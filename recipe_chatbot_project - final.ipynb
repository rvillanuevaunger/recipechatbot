{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Recipe Chatbot Project</center></h1>\n",
    "<h2><center>DATA-641</center></h2>\n",
    "<h3><center>Sobanaa Jayakumar & Reina Villanueva-Unger</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "### A. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import statistics\n",
    "\n",
    "#data processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#feature engg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#model & processing libraries\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn import utils\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#sampling \n",
    "from imblearn import pipeline as pl\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#embeddings\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#DB accesses\n",
    "import sqlite3 as sq\n",
    "\n",
    "#to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#CNN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "#encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#import libraries for chatbot\n",
    "import argparse\n",
    "import logging\n",
    "from typing import Dict\n",
    "from telegram import ReplyKeyboardMarkup, Update, InlineKeyboardMarkup, InlineKeyboardButton\n",
    "from telegram.ext import (Updater, CommandHandler, MessageHandler, Filters, ConversationHandler, CallbackContext, CallbackQueryHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Set Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_path = \"data/cuisine_data/\"\n",
    "recipes_path = \"data/recipes_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/nlp\"\n",
    "model_embeddings_path = os.path.join(model_path, 'similarity_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuisine data\n",
    "def import_cuisine_data(): \n",
    "    train = pd.read_json(os.path.join(cuisine_path, 'train.json'))\n",
    "    return train\n",
    "\n",
    "#Recipe Data \n",
    "def import_recipe_data():\n",
    "    all_recipes = pd.read_json('./data/recipes_data/recipes_raw_nosource_allrecipes.json', orient='index')\n",
    "    epicurious = pd.read_json('./data/recipes_data/recipes_raw_nosource_epicurious.json', orient='index')\n",
    "    food_network = pd.read_json('./data/recipes_data/recipes_raw_nosource_foodnetwork.json', orient='index')\n",
    "    data = pd.concat([all_recipes, epicurious, food_network], axis=0)\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    data = data.drop(columns=['index', 'picture_link'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Set Cuisine Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_classes = ['greek','southern_us','filipino','indian','jamaican','spanish','italian','mexican','chinese','british','thai','vietnamese','cajun_creole','brazilian','french','japanese','irish','korean','moroccan','russian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stop_words = [\"advertisement\", \"advertisements\",\n",
    "                         \"cup\", \"cups\",\n",
    "                         \"tablespoon\", \"tablespoons\", \n",
    "                         \"teaspoon\", \"teaspoons\", \n",
    "                         \"ounce\", \"ounces\",\n",
    "                         \"salt\", \n",
    "                         \"pepper\", \n",
    "                         \"pound\", \"pounds\",\n",
    "                         ]\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.extend(additional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. String Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. String Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text, stemming=False, lemmatize=True, lst_stopwords=None):\n",
    "    #clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "     \n",
    "    lst_text = []    \n",
    "    #tokenize (convert from string to list)\n",
    "    if len(text) > 2:\n",
    "        lst_text = text.split()\n",
    "\n",
    "    #remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    #stemming (remove -ing, -ly, ...)\n",
    "    if stemming == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    #lemmatisation (convert the word into root word)\n",
    "    if lemmatize == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    #back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "\n",
    "    #remove digits\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    #remove mutliple space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Preprocess Cuisine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cuisine():   #cuisine dataset\n",
    "    cuisine_data = import_cuisine_data()\n",
    "\n",
    "    def processing(row):\n",
    "        ls = row['ingredients']\n",
    "        return ' '.join(ls)\n",
    "\n",
    "    cuisine_data['ingredients'] = cuisine_data.apply(lambda x: processing(x), axis=1)\n",
    "    cuisine_data.dropna(inplace=True)\n",
    "    cuisine_data = cuisine_data.drop(columns=['id']).reset_index(drop=True)\n",
    "\n",
    "    cuisine_data[\"ingredients_query\"] = cuisine_data[\"ingredients\"].apply(lambda x: \n",
    "          clean_string(x, stemming=False, lemmatize=True, lst_stopwords=stopword_list))\n",
    "    return cuisine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Preprocess Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recipes(data): # Recipes dataset\n",
    "    data[\"ingredients_query\"] = data[\"ingredients\"].apply(lambda x: \n",
    "            clean_string(x, stemming=False, lemmatize=True, lst_stopwords=stopword_list))\n",
    "    return data\n",
    "\n",
    "def get_tokenize_text(input_text):\n",
    "    return clean_string(input_text, stemming=False, lemmatize=True, lst_stopwords=stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Defining variables - Cuisine subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_subset = process_cuisine().sample(n = 17000)\n",
    "X_s = cuisine_subset['ingredients_query']\n",
    "y_s = cuisine_subset['cuisine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Defining variables - Cuisine full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_data = process_cuisine()\n",
    "X = cuisine_data['ingredients_query']\n",
    "y = cuisine_data['cuisine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cuisine Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Defining vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Defining Test_train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Categorization Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.a. Logistic Regression - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 500, random_state = 123, multi_class = 'multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 10\n",
      "Best Solver: newton-cg\n"
     ]
    }
   ],
   "source": [
    "#Assigning values to params \n",
    "penalty = ['l2', 'l1', 'elasticnet']\n",
    "C = [0.001, 0.01, 0.1, 10]\n",
    "solvers = ['newton-cg', 'lbfgs']\n",
    "\n",
    "#Fitting into pipeline\n",
    "lr_pipe = Pipeline([('vect', vectorizer), \n",
    "                    ('lr', lr)])\n",
    "\n",
    "#Create hyperparameter dict\n",
    "hyperparameters = dict(lr__C = C, lr__penalty = penalty, lr__solver = solvers)\n",
    "\n",
    "#Grid search\n",
    "clf = GridSearchCV(lr_pipe, hyperparameters, cv = 5)\n",
    "best_model = clf.fit(X, y)\n",
    "\n",
    "#Printing best params\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['lr__penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['lr__C'])\n",
    "print('Best Solver:', best_model.best_estimator_.get_params()['lr__solver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.b. Logistic Regression - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score weighted 0.7834724049206725\n",
      "Accuracy score 0.787308930008045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.80      0.57      0.67       117\n",
      "     british       0.61      0.45      0.52       201\n",
      "cajun_creole       0.78      0.69      0.73       386\n",
      "     chinese       0.81      0.87      0.84       668\n",
      "    filipino       0.79      0.57      0.66       189\n",
      "      french       0.58      0.67      0.62       662\n",
      "       greek       0.77      0.69      0.72       294\n",
      "      indian       0.87      0.91      0.89       751\n",
      "       irish       0.69      0.49      0.57       167\n",
      "     italian       0.81      0.89      0.84      1960\n",
      "    jamaican       0.92      0.71      0.80       131\n",
      "    japanese       0.85      0.69      0.76       356\n",
      "      korean       0.86      0.75      0.80       207\n",
      "     mexican       0.91      0.91      0.91      1610\n",
      "    moroccan       0.81      0.77      0.79       205\n",
      "     russian       0.65      0.45      0.53       122\n",
      " southern_us       0.72      0.81      0.76      1080\n",
      "     spanish       0.61      0.47      0.53       247\n",
      "        thai       0.74      0.81      0.77       385\n",
      "  vietnamese       0.68      0.48      0.56       206\n",
      "\n",
      "    accuracy                           0.79      9944\n",
      "   macro avg       0.76      0.68      0.71      9944\n",
      "weighted avg       0.79      0.79      0.78      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting vectorizer\n",
    "matrix_train_lr = vectorizer.fit_transform(X_train)\n",
    "matrix_test_lr = vectorizer.transform(X_test)\n",
    "\n",
    "#Fitting final model with Best hyper params\n",
    "lr_final = LogisticRegression(max_iter = 300, random_state = 123, multi_class ='multinomial', solver = 'newton-cg', C = 10, penalty = 'l2')\n",
    "lr_final.fit(matrix_train_lr, y_train)\n",
    "y_pred = lr_final.predict(matrix_test_lr)\n",
    "pred_prob = lr_final.predict_proba(matrix_test_lr)\n",
    "\n",
    "#Classification metrics\n",
    "print('f1 score weighted %s' % f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "acc_lr = accuracy_score(y_test, y_pred)\n",
    "f1_lr = f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ii.a. Multinomial Naive Bayes - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "#Assigning values to params\n",
    "alpha = [0.01, 0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "#Fitting into pipeline\n",
    "nb_pipe = Pipeline([('vect', vectorizer), \n",
    "                    ('nb', nb)])\n",
    "\n",
    "#Create hyperparameter dict\n",
    "hyperparameters = dict(nb__alpha = alpha)\n",
    "\n",
    "#Grid search\n",
    "clf = GridSearchCV(nb_pipe, hyperparameters, cv = 5)\n",
    "best_model = clf.fit(X, y)\n",
    "\n",
    "#Printing best params\n",
    "print('Best Alpha:', best_model.best_estimator_.get_params()['nb__alpha'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.b. Multinomial Naive Bayes - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score weighted 0.736116043309851\n",
      "Accuracy score 0.7412510056315366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.71      0.47      0.57       117\n",
      "     british       0.52      0.41      0.46       201\n",
      "cajun_creole       0.66      0.69      0.68       386\n",
      "     chinese       0.76      0.88      0.81       668\n",
      "    filipino       0.81      0.49      0.61       189\n",
      "      french       0.52      0.60      0.56       662\n",
      "       greek       0.74      0.56      0.64       294\n",
      "      indian       0.84      0.87      0.86       751\n",
      "       irish       0.71      0.44      0.54       167\n",
      "     italian       0.78      0.85      0.81      1960\n",
      "    jamaican       0.85      0.58      0.69       131\n",
      "    japanese       0.85      0.64      0.73       356\n",
      "      korean       0.88      0.65      0.75       207\n",
      "     mexican       0.88      0.88      0.88      1610\n",
      "    moroccan       0.76      0.70      0.73       205\n",
      "     russian       0.66      0.34      0.45       122\n",
      " southern_us       0.63      0.75      0.68      1080\n",
      "     spanish       0.58      0.37      0.45       247\n",
      "        thai       0.66      0.78      0.71       385\n",
      "  vietnamese       0.63      0.43      0.51       206\n",
      "\n",
      "    accuracy                           0.74      9944\n",
      "   macro avg       0.72      0.62      0.66      9944\n",
      "weighted avg       0.74      0.74      0.74      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting vectorizer\n",
    "matrix_train = vectorizer.fit_transform(X_train)\n",
    "matrix_test = vectorizer.transform(X_test)\n",
    "\n",
    "nb_final = MultinomialNB(alpha = 0.01)\n",
    "nb_final.fit(matrix_train, y_train)\n",
    "y_pred = nb_final.predict(matrix_test)\n",
    "\n",
    "#Classification metrics\n",
    "print('f1 score weighted %s' % f1_score(y_test,y_pred, average ='weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "acc_mnb = accuracy_score(y_test, y_pred)\n",
    "f1_mnb = f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.a. Random Forest - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: 500\n",
      "Best features: sqrt\n",
      "Best samples split: 2\n",
      "Best samples leaf: 5\n"
     ]
    }
   ],
   "source": [
    "#Assigning values to params \n",
    "n_estimators = [10, 100, 500]\n",
    "max_features = ['sqrt', 'log2']\n",
    "min_samples_split = [2, 10, 100]\n",
    "min_samples_leaf = [5, 10] \n",
    "\n",
    "\n",
    "#Fitting into pipeline\n",
    "rf_pipe = Pipeline([('vect', vectorizer), \n",
    "                    ('rf', rf)])\n",
    "\n",
    "#Create hyperparameter dict\n",
    "hyperparameters = dict(rf__n_estimators = estimators, rf__max_features = max_features, rf__min_samples_split = min_samples_split, rf__min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "#Grid search\n",
    "clf = GridSearchCV(rf_pipe, hyperparameters, cv = 5)\n",
    "best_model = clf.fit(X, y)\n",
    "\n",
    "#Printing best params\n",
    "print('Best Estimator:', best_model.best_estimator_.get_params()['rf__n_estimators'])\n",
    "print('Best features:', best_model.best_estimator_.get_params()['rf__max_features'])\n",
    "print('Best samples split:', best_model.best_estimator_.get_params()['rf__min_samples_split'])\n",
    "print('Best samples leaf:', best_model.best_estimator_.get_params()['rf__min_samples_leaf'])\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.b. Random Forest - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score weighted 0.5706338276541044\n",
      "Accuracy score 0.6288213998390989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.85      0.24      0.37       117\n",
      "     british       1.00      0.00      0.01       201\n",
      "cajun_creole       0.89      0.49      0.63       386\n",
      "     chinese       0.61      0.90      0.72       668\n",
      "    filipino       1.00      0.07      0.14       189\n",
      "      french       0.58      0.20      0.30       662\n",
      "       greek       0.93      0.23      0.37       294\n",
      "      indian       0.78      0.86      0.82       751\n",
      "       irish       0.80      0.02      0.05       167\n",
      "     italian       0.56      0.92      0.69      1960\n",
      "    jamaican       1.00      0.12      0.22       131\n",
      "    japanese       0.96      0.46      0.62       356\n",
      "      korean       0.98      0.20      0.34       207\n",
      "     mexican       0.73      0.91      0.81      1610\n",
      "    moroccan       0.98      0.21      0.35       205\n",
      "     russian       0.00      0.00      0.00       122\n",
      " southern_us       0.47      0.71      0.56      1080\n",
      "     spanish       0.00      0.00      0.00       247\n",
      "        thai       0.68      0.66      0.67       385\n",
      "  vietnamese       0.76      0.08      0.14       206\n",
      "\n",
      "    accuracy                           0.63      9944\n",
      "   macro avg       0.73      0.36      0.39      9944\n",
      "weighted avg       0.67      0.63      0.57      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting vectorizer\n",
    "matrix_train_rf = vectorizer.fit_transform(X_train)\n",
    "matrix_test_rf = vectorizer.transform(X_test)\n",
    "\n",
    "#Fitting final model with Best hyper params\n",
    "rf_final = RandomForestClassifier(random_state = 123, n_estimators = 500, max_features = 'sqrt', min_samples_split = 2, min_samples_leaf = 5)\n",
    "rf_final.fit(matrix_train_rf, y_train)\n",
    "y_pred = rf_final.predict(matrix_test_rf)\n",
    "\n",
    "#Classification metrics\n",
    "print('f1 score weighted %s' % f1_score(y_test,y_pred, average ='weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "f1_rf = f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv.a. SVC - Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Kernel: sigmoid\n",
      "Best C: 10\n"
     ]
    }
   ],
   "source": [
    "#Assigning values to params \n",
    "kernel = ['rbf', 'poly', 'sigmoid']  \n",
    "C = [0.001, 0.01, 0.1, 10]\n",
    "\n",
    "#Fitting into pipeline\n",
    "svc_pipe = Pipeline([('vect', vectorizer),\n",
    "                          ('svc', svc)])\n",
    "\n",
    "#Create hyperparameter dict\n",
    "hyperparameters = dict(svc__kernel = kernel, svc__C = C)\n",
    "\n",
    "#Grid search\n",
    "clf = GridSearchCV(svc_pipe, hyperparameters, cv = 5)\n",
    "best_model = clf.fit(X_s, y_s)\n",
    "\n",
    "#Printing best params\n",
    "print('Best Kernel:', best_model.best_estimator_.get_params()['svc__kernel'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['svc__C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv.b. SVC - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score weighted 0.7521964647011117\n",
      "Accuracy score 0.7526146419951729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.67      0.65      0.66       117\n",
      "     british       0.49      0.53      0.51       201\n",
      "cajun_creole       0.71      0.69      0.70       386\n",
      "     chinese       0.79      0.84      0.81       668\n",
      "    filipino       0.67      0.61      0.64       189\n",
      "      french       0.50      0.64      0.56       662\n",
      "       greek       0.68      0.69      0.68       294\n",
      "      indian       0.85      0.88      0.86       751\n",
      "       irish       0.51      0.47      0.49       167\n",
      "     italian       0.80      0.82      0.81      1960\n",
      "    jamaican       0.85      0.76      0.80       131\n",
      "    japanese       0.76      0.68      0.72       356\n",
      "      korean       0.84      0.74      0.78       207\n",
      "     mexican       0.89      0.89      0.89      1610\n",
      "    moroccan       0.77      0.76      0.77       205\n",
      "     russian       0.57      0.41      0.48       122\n",
      " southern_us       0.73      0.70      0.71      1080\n",
      "     spanish       0.55      0.43      0.49       247\n",
      "        thai       0.76      0.78      0.77       385\n",
      "  vietnamese       0.62      0.50      0.55       206\n",
      "\n",
      "    accuracy                           0.75      9944\n",
      "   macro avg       0.70      0.67      0.68      9944\n",
      "weighted avg       0.75      0.75      0.75      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting vectorizer\n",
    "matrix_train = vectorizer.fit_transform(X_train)\n",
    "matrix_test = vectorizer.transform(X_test)\n",
    "\n",
    "#Fitting final model with Best hyper params\n",
    "svc_final = SVC(random_state = 123, kernel = 'sigmoid', C = 10)\n",
    "svc_final.fit(matrix_train, y_train)\n",
    "y_pred = svc_final.predict(matrix_test)\n",
    "\n",
    "#Classification metrics\n",
    "print('f1 score weighted %s' % f1_score(y_test,y_pred, average='weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "acc_svm = accuracy_score(y_test, y_pred)\n",
    "f1_svm = f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v.a. XGB - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective = 'multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [1, 5, 20, 50]\n",
    "gamma = [1, 10]\n",
    "reg_alpha = [1, 40, 180]\n",
    "reg_lambda = [0, 1]\n",
    "colsample_bytree = [0.5, 1]\n",
    "min_child_weight = [0, 1, 10]\n",
    "eta = [0.5, 1]\n",
    "\n",
    "\n",
    "xgb_pipe = Pipeline([('vect', vectorizer),\n",
    "                      ('xgb', xgb_model)])\n",
    "\n",
    "#Create hyperparameter dict\n",
    "hyperparameters = dict(xgb__max_depth = max_depth, xgb__gamma = gamma, xgb__reg_alpha = reg_alpha, \n",
    "                       xgb__reg_lambda = reg_lambda, xgb__colsample_bytree = colsample_bytree, xgb__min_child_weight = min_child_weight, xgb__eta = eta)\n",
    "\n",
    "#Grid search\n",
    "clf = GridSearchCV(xgb_pipe, hyperparameters, cv = 5)\n",
    "best_model = clf.fit(X_s, y_s)\n",
    "\n",
    "#Printing best params\n",
    "print('Best Depth:', best_model.best_estimator_.get_params()['xgb__max_depth'])\n",
    "print('Best Gamma:', best_model.best_estimator_.get_params()['xgb__gamma'])\n",
    "print('Best Alpha:', best_model.best_estimator_.get_params()['xgb__reg_alpha'])\n",
    "print('Best Lambda:', best_model.best_estimator_.get_params()['xgb__reg_lambda'])\n",
    "print('Best Colsample:', best_model.best_estimator_.get_params()['xgb__colsample_bytree'])\n",
    "print('Best Child weight:', best_model.best_estimator_.get_params()['xgb__min_child_weight'])\n",
    "print('Best ETA:', best_model.best_estimator_.get_params()['xgb__eta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v.b. XGB - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score weighted 0.7377902089531876\n",
      "Accuracy score 0.7453740949316171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.78      0.53      0.63       117\n",
      "     british       0.50      0.24      0.33       201\n",
      "cajun_creole       0.79      0.67      0.73       386\n",
      "     chinese       0.78      0.84      0.81       668\n",
      "    filipino       0.68      0.48      0.57       189\n",
      "      french       0.54      0.55      0.54       662\n",
      "       greek       0.81      0.70      0.75       294\n",
      "      indian       0.86      0.86      0.86       751\n",
      "       irish       0.62      0.43      0.50       167\n",
      "     italian       0.70      0.88      0.78      1960\n",
      "    jamaican       0.89      0.67      0.77       131\n",
      "    japanese       0.85      0.65      0.73       356\n",
      "      korean       0.79      0.66      0.72       207\n",
      "     mexican       0.88      0.89      0.89      1610\n",
      "    moroccan       0.81      0.71      0.76       205\n",
      "     russian       0.58      0.34      0.43       122\n",
      " southern_us       0.66      0.76      0.70      1080\n",
      "     spanish       0.63      0.35      0.45       247\n",
      "        thai       0.75      0.79      0.77       385\n",
      "  vietnamese       0.68      0.43      0.53       206\n",
      "\n",
      "    accuracy                           0.75      9944\n",
      "   macro avg       0.73      0.62      0.66      9944\n",
      "weighted avg       0.74      0.75      0.74      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting vectorizer\n",
    "matrix_train = vectorizer.fit_transform(X_train)\n",
    "matrix_test = vectorizer.transform(X_test)\n",
    "\n",
    "#Fitting final model with Best hyper params. Import xgb before running this\n",
    "xgb_final = xgb.XGBClassifier(objective = 'multi:softmax', max_depth = 1, gamma = 1, reg_alpha = 1, \n",
    "                     reg_lambda = 0, colsample_bytree = 0.5, min_child_weight = 1, eta = 0.5, eval_metric = 'mlogloss')\n",
    "\n",
    "xgb_final.fit(matrix_train, y_train)\n",
    "y_pred = xgb_final.predict(matrix_test)\n",
    "\n",
    "#Classification metrics\n",
    "print('f1 score weighted %s' % f1_score(y_test,y_pred, average='weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, y_pred)\n",
    "f1_xgb = f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi.a. Over and Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_samp = LogisticRegression(max_iter = 300, random_state = 123, multi_class = 'multinomial',  solver = 'newton-cg', C = 10, penalty = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_os = RandomOverSampler(random_state=123)\n",
    "random_us = RandomUnderSampler(random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi.b. Oversampling LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results on oversampling:\n",
      "\n",
      "f1 score weighted 0.7856498439972385\n",
      "Accuracy score 0.7844931617055511\n",
      "                    pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "   brazilian       0.70      0.68      1.00      0.69      0.82      0.65       117\n",
      "     british       0.53      0.57      0.99      0.55      0.75      0.54       201\n",
      "cajun_creole       0.74      0.75      0.99      0.74      0.86      0.72       386\n",
      "     chinese       0.82      0.85      0.99      0.83      0.91      0.82       668\n",
      "    filipino       0.72      0.65      0.99      0.68      0.80      0.63       189\n",
      "      french       0.58      0.68      0.96      0.62      0.81      0.63       662\n",
      "       greek       0.71      0.77      0.99      0.74      0.87      0.74       294\n",
      "      indian       0.89      0.90      0.99      0.90      0.95      0.89       751\n",
      "       irish       0.57      0.57      0.99      0.57      0.75      0.54       167\n",
      "     italian       0.85      0.83      0.96      0.84      0.89      0.79      1960\n",
      "    jamaican       0.83      0.79      1.00      0.81      0.89      0.78       131\n",
      "    japanese       0.83      0.72      0.99      0.77      0.85      0.70       356\n",
      "      korean       0.81      0.79      1.00      0.80      0.89      0.77       207\n",
      "     mexican       0.93      0.90      0.99      0.91      0.94      0.88      1610\n",
      "    moroccan       0.74      0.81      0.99      0.78      0.90      0.79       205\n",
      "     russian       0.55      0.52      0.99      0.54      0.72      0.50       122\n",
      " southern_us       0.76      0.77      0.97      0.77      0.86      0.73      1080\n",
      "     spanish       0.52      0.52      0.99      0.52      0.72      0.49       247\n",
      "        thai       0.74      0.78      0.99      0.76      0.88      0.76       385\n",
      "  vietnamese       0.62      0.54      0.99      0.58      0.73      0.52       206\n",
      "\n",
      " avg / total       0.79      0.78      0.98      0.79      0.88      0.76      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_pipe_os = pl.make_pipeline(vectorizer,\n",
    "                           random_os,\n",
    "                           lr_samp)\n",
    "\n",
    "\n",
    "#Train the classifier with balancing\n",
    "lr_pipe_os.fit(X_train, y_train)\n",
    "\n",
    "#Test the classifier and get the prediction\n",
    "y_pred_bal = lr_pipe_os.predict(X_test)\n",
    "\n",
    "print(\"results on oversampling:\\n\")\n",
    "print('f1 score weighted %s' % f1_score(y_test, y_pred_bal, average = 'weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred_bal))\n",
    "print(classification_report_imbalanced(y_test, y_pred_bal))\n",
    "\n",
    "acc_os_lr = accuracy_score(y_test, y_pred_bal)\n",
    "f1_os_lr = f1_score(y_test, y_pred_bal, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi.c. Undersampling LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results on Undersampling:\n",
      "\n",
      "f1 score weighted 0.7209825708033166\n",
      "Accuracy score 0.7059533386967015\n",
      "                    pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "   brazilian       0.41      0.76      0.99      0.54      0.87      0.73       117\n",
      "     british       0.31      0.61      0.97      0.41      0.77      0.57       201\n",
      "cajun_creole       0.63      0.76      0.98      0.69      0.86      0.73       386\n",
      "     chinese       0.85      0.77      0.99      0.81      0.87      0.75       668\n",
      "    filipino       0.49      0.68      0.99      0.57      0.82      0.65       189\n",
      "      french       0.51      0.54      0.96      0.52      0.72      0.49       662\n",
      "       greek       0.60      0.79      0.98      0.68      0.88      0.76       294\n",
      "      indian       0.91      0.83      0.99      0.87      0.91      0.81       751\n",
      "       irish       0.35      0.63      0.98      0.45      0.79      0.59       167\n",
      "     italian       0.89      0.67      0.98      0.76      0.81      0.63      1960\n",
      "    jamaican       0.62      0.83      0.99      0.71      0.91      0.81       131\n",
      "    japanese       0.80      0.67      0.99      0.73      0.82      0.65       356\n",
      "      korean       0.70      0.77      0.99      0.74      0.88      0.75       207\n",
      "     mexican       0.95      0.81      0.99      0.87      0.90      0.79      1610\n",
      "    moroccan       0.60      0.84      0.99      0.70      0.91      0.82       205\n",
      "     russian       0.33      0.67      0.98      0.44      0.81      0.64       122\n",
      " southern_us       0.79      0.58      0.98      0.67      0.75      0.55      1080\n",
      "     spanish       0.34      0.60      0.97      0.43      0.76      0.56       247\n",
      "        thai       0.75      0.73      0.99      0.74      0.85      0.70       385\n",
      "  vietnamese       0.53      0.64      0.99      0.58      0.79      0.61       206\n",
      "\n",
      " avg / total       0.76      0.71      0.98      0.72      0.83      0.68      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_pipe_us = pl.make_pipeline(vectorizer,\n",
    "                           random_us,\n",
    "                           lr_samp)\n",
    "\n",
    "\n",
    "\n",
    "#Train the classifier with balancing\n",
    "lr_pipe_us.fit(X_train, y_train)\n",
    "\n",
    "#Test the classifier and get the prediction\n",
    "y_pred_bal = lr_pipe_us.predict(X_test)\n",
    "\n",
    "print(\"results on Undersampling:\\n\")\n",
    "print('f1 score weighted %s' % f1_score(y_test, y_pred_bal, average = 'weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred_bal))\n",
    "print(classification_report_imbalanced(y_test, y_pred_bal))\n",
    "\n",
    "acc_us_lr = accuracy_score(y_test, y_pred_bal)\n",
    "f1_us_lr = f1_score(y_test, y_pred_bal, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vii. CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "train_texts = X_train\n",
    "test_texts = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining parameters for keras\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 50000 \n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion of label data\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2661 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) \n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the train data into train and valid is done\n"
     ]
    }
   ],
   "source": [
    "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "trainvalid_labels = to_categorical(train_labels, num_classes = 20, dtype =\"int32\")\n",
    "test_labels = to_categorical(test_labels, num_classes = 20, dtype =\"int32\")\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "x_train = trainvalid_data[:-num_validation_samples]\n",
    "y_train = trainvalid_labels[:-num_validation_samples]\n",
    "x_val = trainvalid_data[-num_validation_samples:]\n",
    "y_val = trainvalid_labels[-num_validation_samples:]\n",
    "\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "#Training our own embeddings\n",
    "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
    "cnnmodel_own = Sequential()\n",
    "cnnmodel_own.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnnmodel_own.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel_own.add(MaxPooling1D(5))\n",
    "cnnmodel_own.add(Dropout(0.5))\n",
    "cnnmodel_own.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel_own.add(MaxPooling1D(5))\n",
    "cnnmodel_own.add(Dropout(0.5))\n",
    "cnnmodel_own.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel_own.add(GlobalMaxPooling1D())\n",
    "cnnmodel_own.add(Dense(128, activation='relu'))\n",
    "cnnmodel_own.add(Dropout(0.5))\n",
    "cnnmodel_own.add(Dense(len(trainvalid_labels[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1492/1492 [==============================] - 872s 580ms/step - loss: 1.0888 - acc: 0.6739 - f1_m: 0.6579 - precision_m: 0.8457 - recall_m: 0.5459 - val_loss: 1.1350 - val_acc: 0.6664 - val_f1_m: 0.6569 - val_precision_m: 0.8514 - val_recall_m: 0.5419\n",
      "Epoch 2/5\n",
      "1492/1492 [==============================] - 785s 526ms/step - loss: 1.0483 - acc: 0.6830 - f1_m: 0.6688 - precision_m: 0.8467 - recall_m: 0.5597 - val_loss: 1.1056 - val_acc: 0.6765 - val_f1_m: 0.6568 - val_precision_m: 0.8700 - val_recall_m: 0.5347\n",
      "Epoch 3/5\n",
      "1492/1492 [==============================] - 570s 382ms/step - loss: 1.0183 - acc: 0.6928 - f1_m: 0.6774 - precision_m: 0.8512 - recall_m: 0.5700 - val_loss: 1.1091 - val_acc: 0.6812 - val_f1_m: 0.6640 - val_precision_m: 0.8625 - val_recall_m: 0.5471\n",
      "Epoch 4/5\n",
      "1492/1492 [==============================] - 631s 423ms/step - loss: 0.9824 - acc: 0.7024 - f1_m: 0.6891 - precision_m: 0.8537 - recall_m: 0.5850 - val_loss: 1.1012 - val_acc: 0.6827 - val_f1_m: 0.6727 - val_precision_m: 0.8522 - val_recall_m: 0.5628\n",
      "Epoch 5/5\n",
      "1492/1492 [==============================] - 703s 471ms/step - loss: 0.9556 - acc: 0.7099 - f1_m: 0.7002 - precision_m: 0.8560 - recall_m: 0.5996 - val_loss: 1.1207 - val_acc: 0.6768 - val_f1_m: 0.6655 - val_precision_m: 0.8538 - val_recall_m: 0.5526\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the model\n",
    "cnnmodel_own.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "history = cnnmodel_own.fit(x_train, y_train,\n",
    "                            batch_size = 16, \n",
    "                            epochs = 5, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = cnnmodel_own.evaluate(test_data, test_labels, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6854384541511536\n",
      "f1 score: 0.671891450881958\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {accuracy}')\n",
    "print(f'f1 score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vii. a. CNN - Training and Validation accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmZ0lEQVR4nO3deXxV1b338c+PmQg4ACoSQ6BlnhIIg4AUh7ag1FLEqzRFEa8D9tYqrdVqLXSgt09L+/LyWKU41hqlPg5cxbGoiDhUw6CCgnUgNBUVURkElOH3/LFOSEhOkhM4yU52vu/XK6+cs886+/yyA9+ss/Y+a5m7IyIiDV+TqAsQEZH0UKCLiMSEAl1EJCYU6CIiMaFAFxGJCQW6iEhMKNAlKTN7zMzOS3fbKJnZejM7tRb262b21cTteWZ2XSptD+J18s3syYOts4r9jjGz4nTvV+pes6gLkPQxs+1l7mYAXwB7E/cvdveCVPfl7uNqo23cufsl6diPmWUD7wHN3X1PYt8FQMq/Q2l8FOgx4u5tSm6b2XrgP919cfl2ZtasJCREJD405NIIlLylNrOrzOwD4HYzO9LMFpnZJjP7NHE7s8xzlpjZfyZuTzWzZWY2J9H2PTMbd5Btu5rZUjPbZmaLzexPZnZXJXWnUuOvzOz5xP6eNLMOZR6fYmZFZrbZzK6t4vgMN7MPzKxpmW3fMbPXEreHmtmLZvaZmW00sxvMrEUl+7rDzH5d5v6Viee8b2bTyrU93cxWmtlWM/uXmc0q8/DSxPfPzGy7mZ1QcmzLPH+Emb1iZlsS30ekemyqYma9E8//zMzWmNkZZR47zczeSOzz32b248T2Donfz2dm9omZPWdmypc6pgPeeBwLHAV0AS4i/O5vT9zPAnYCN1Tx/GHAOqAD8DvgVjOzg2h7N/Ay0B6YBUyp4jVTqfG7wPnA0UALoCRg+gA3JfZ/XOL1MknC3V8CPgdOLrffuxO39wJXJH6eE4BTgEurqJtEDWMT9Xwd6A6UH7//HDgXOAI4HZhuZhMSj41OfD/C3du4+4vl9n0U8AgwN/Gz/RF4xMzal/sZKhybampuDjwMPJl43g+AAjPrmWhyK2H4ri3QD3g6sf1HQDHQETgGuAbQvCJ1TIHeeOwDZrr7F+6+0903u/v97r7D3bcBs4GvVfH8Ine/2d33An8BOhH+46bc1syygCHAz939S3dfBjxU2QumWOPt7v6Wu+8E7gVyEtsnAYvcfam7fwFclzgGlbkHmAxgZm2B0xLbcPfl7v6Su+9x9/XAn5PUkcx/JOpb7e6fE/6Alf35lrj76+6+z91fS7xeKvuF8Afgn+7+10Rd9wBrgW+VaVPZsanKcKAN8NvE7+hpYBGJYwPsBvqYWTt3/9TdV5TZ3gno4u673f0510RRdU6B3nhscvddJXfMLMPM/pwYkthKeIt/RNlhh3I+KLnh7jsSN9vUsO1xwCdltgH8q7KCU6zxgzK3d5Sp6biy+04E6ubKXovQG59oZi2BicAKdy9K1NEjMZzwQaKO3xB669U5oAagqNzPN8zMnkkMKW0BLklxvyX7Liq3rQjoXOZ+Zcem2prdvewfv7L7PZPwx67IzJ41sxMS238PvA08aWbvmtnVqf0Ykk4K9MajfG/pR0BPYJi7t6P0LX5lwyjpsBE4yswyymw7vor2h1LjxrL7Trxm+8oau/sbhOAax4HDLRCGbtYC3RN1XHMwNRCGjcq6m/AO5Xh3PxyYV2a/1fVu3ycMRZWVBfw7hbqq2+/x5ca/9+/X3V9x928ThmMWEnr+uPs2d/+Ru3cjvEuYYWanHGItUkMK9MarLWFM+rPEeOzM2n7BRI+3EJhlZi0SvbtvVfGUQ6nxPmC8mY1KnMD8JdX/e78buIzwh+P/latjK7DdzHoB01Os4V5gqpn1SfxBKV9/W8I7ll1mNpTwh6TEJsIQUbdK9v0o0MPMvmtmzczsbKAPYXjkUPyDMLb/EzNrbmZjCL+jBYnfWb6ZHe7uuwnHZC+AmY03s68mzpWUbN+b9BWk1ijQG6/rgdbAx8BLwON19Lr5hBOLm4FfA38jXC+fzPUcZI3uvgb4PiGkNwKfEk7aVeUeYAzwtLt/XGb7jwlhuw24OVFzKjU8lvgZniYMRzxdrsmlwC/NbBvwcxK93cRzdxDOGTyfuHJkeLl9bwbGE97FbAZ+AowvV3eNufuXwBmEdyofAzcC57r72kSTKcD6xNDTJcD3Etu7A4uB7cCLwI3uvuRQapGaM523kCiZ2d+Ate5e6+8QROJOPXSpU2Y2xMy+YmZNEpf1fZswFisih0ifFJW6dizwAOEEZTEw3d1XRluSSDxoyEVEJCY05CIiEhORDbl06NDBs7Ozo3p5EZEGafny5R+7e8dkj0UW6NnZ2RQWFkb18iIiDZKZlf+E8H4achERiQkFuohITCjQRURiQtehizQiu3fvpri4mF27dlXfWCLVqlUrMjMzad68ecrPUaCLNCLFxcW0bduW7OxsKl+fRKLm7mzevJni4mK6du2a8vOqHXIxs+MTcza/mViO6odJ2piZzTWzt83sNTMbVMP6U1JQANnZ0KRJ+F6g5XJFamTXrl20b99eYV7PmRnt27ev8TupVHroe4AfufuKxEouy83s74n5o0uMI8y21p2w/NhNie9pU1AAF10EOxJLIxQVhfsA+fnpfCWReFOYNwwH83uqtofu7htLlplKLAP2JgeuigJhgqU7PXiJsKpMpxpXU4Vrry0N8xI7doTtIiJSw6tczCwbyCVMgl9WZw5caquYiqGPmV1kZoVmVrhp06YaFbphQ822i0j9s3nzZnJycsjJyeHYY4+lc+fO++9/+eWXVT63sLCQyy67rNrXGDFiRFpqXbJkCePHj0/LvupKyoFuZm2A+4HL3X1r+YeTPKXCrF/uPt/d89w9r2PHpJ9crVRW+cW7qtkuIocu3eet2rdvz6pVq1i1ahWXXHIJV1xxxf77LVq0YM+ePZU+Ny8vj7lz51b7Gi+88MKhFdmApRToZtacEOYF7v5AkibFHLh2YiZhbcK0mT0bMjIO3JaREbaLSPqVnLcqKgL30vNW6b4YYerUqcyYMYOTTjqJq666ipdffpkRI0aQm5vLiBEjWLduHXBgj3nWrFlMmzaNMWPG0K1btwOCvk2bNvvbjxkzhkmTJtGrVy/y8/MpmV320UcfpVevXowaNYrLLrus2p74J598woQJExgwYADDhw/ntddeA+DZZ5/d/w4jNzeXbdu2sXHjRkaPHk1OTg79+vXjueeeS+8Bq0K1J0UTawTeCrzp7n+spNlDwH+Z2QLCydAt7r4xfWWWnvi89towzJKVFcJcJ0RFakdV563S/f/urbfeYvHixTRt2pStW7eydOlSmjVrxuLFi7nmmmu4//77Kzxn7dq1PPPMM2zbto2ePXsyffr0Ctdsr1y5kjVr1nDccccxcuRInn/+efLy8rj44otZunQpXbt2ZfLkydXWN3PmTHJzc1m4cCFPP/005557LqtWrWLOnDn86U9/YuTIkWzfvp1WrVoxf/58vvnNb3Lttdeyd+9edpQ/iLUolatcRhLWEXzdzFYltl1DYgVzd59HWLD2NMK6iTuA89NeKeEfkQJcpG7U5Xmrs846i6ZNmwKwZcsWzjvvPP75z39iZuzevTvpc04//XRatmxJy5YtOfroo/nwww/JzMw8oM3QoUP3b8vJyWH9+vW0adOGbt267b++e/LkycyfP7/K+pYtW7b/j8rJJ5/M5s2b2bJlCyNHjmTGjBnk5+czceJEMjMzGTJkCNOmTWP37t1MmDCBnJycQzk0NZLKVS7L3N3cfYC75yS+HnX3eYkwJ3F1y/fd/Svu3t/dNY2iSANXl+etDjvssP23r7vuOk466SRWr17Nww8/XOm12C1bttx/u2nTpknH35O1OZhFfZI9x8y4+uqrueWWW9i5cyfDhw9n7dq1jB49mqVLl9K5c2emTJnCnXfeWePXO1iay0VEkorqvNWWLVvo3DlcJHfHHXekff+9evXi3XffZf369QD87W9/q/Y5o0ePpiBx8mDJkiV06NCBdu3a8c4779C/f3+uuuoq8vLyWLt2LUVFRRx99NFceOGFXHDBBaxYsSLtP0NlFOgiklR+PsyfD126gFn4Pn9+7Q97/uQnP+GnP/0pI0eOZO/evWnff+vWrbnxxhsZO3Yso0aN4phjjuHwww+v8jmzZs2isLCQAQMGcPXVV/OXv/wFgOuvv55+/foxcOBAWrduzbhx41iyZMn+k6T3338/P/xhhQ/X15rI1hTNy8tzLXAhUrfefPNNevfuHXUZkdu+fTtt2rTB3fn+979P9+7dueKKK6Iuq4Jkvy8zW+7uecnaq4cuIo3OzTffTE5ODn379mXLli1cfPHFUZeUFpptUUQanSuuuKJe9sgPlXroIiIxoUAXEYkJBbqISEwo0EVEYkKBLiJ1ZsyYMTzxxBMHbLv++uu59NJLq3xOySXOp512Gp999lmFNrNmzWLOnDlVvvbChQt5443SdXl+/vOfs3jx4hpUn1x9mmZXgS4idWby5MksWLDggG0LFixIaYIsCLMkHnHEEQf12uUD/Ze//CWnnnrqQe2rvlKgi0idmTRpEosWLeKLL74AYP369bz//vuMGjWK6dOnk5eXR9++fZk5c2bS52dnZ/Pxxx8DMHv2bHr27Mmpp566f4pdCNeYDxkyhIEDB3LmmWeyY8cOXnjhBR566CGuvPJKcnJyeOedd5g6dSr33XcfAE899RS5ubn079+fadOm7a8vOzubmTNnMmjQIPr378/atWur/PminmZX16GLNFKXXw6rVqV3nzk5cP31lT/evn17hg4dyuOPP863v/1tFixYwNlnn42ZMXv2bI466ij27t3LKaecwmuvvcaAAQOS7mf58uUsWLCAlStXsmfPHgYNGsTgwYMBmDhxIhdeeCEAP/vZz7j11lv5wQ9+wBlnnMH48eOZNGnSAfvatWsXU6dO5amnnqJHjx6ce+653HTTTVx++eUAdOjQgRUrVnDjjTcyZ84cbrnllkp/vqin2VUPXUTqVNlhl7LDLffeey+DBg0iNzeXNWvWHDA8Ut5zzz3Hd77zHTIyMmjXrh1nnHHG/sdWr17NiSeeSP/+/SkoKGDNmjVV1rNu3Tq6du1Kjx49ADjvvPNYunTp/scnTpwIwODBg/dP6FWZZcuWMWXKFCD5NLtz587ls88+o1mzZgwZMoTbb7+dWbNm8frrr9O2bdsq950K9dBFGqmqetK1acKECcyYMYMVK1awc+dOBg0axHvvvcecOXN45ZVXOPLII5k6dWql0+aWCGvvVDR16lQWLlzIwIEDueOOO1iyZEmV+6luPquSKXgrm6K3un2VTLN7+umn8+ijjzJ8+HAWL168f5rdRx55hClTpnDllVdy7rnnVrn/6qiHLiJ1qk2bNowZM4Zp06bt751v3bqVww47jMMPP5wPP/yQxx57rMp9jB49mgcffJCdO3eybds2Hn744f2Pbdu2jU6dOrF79+79U94CtG3blm3btlXYV69evVi/fj1vv/02AH/961/52te+dlA/W9TT7KqHLiJ1bvLkyUycOHH/0MvAgQPJzc2lb9++dOvWjZEjR1b5/EGDBnH22WeTk5NDly5dOPHEE/c/9qtf/Yphw4bRpUsX+vfvvz/EzznnHC688ELmzp27/2QoQKtWrbj99ts566yz2LNnD0OGDOGSSy45qJ9r1qxZnH/++QwYMICMjIwDptl95plnaNq0KX369GHcuHEsWLCA3//+9zRv3pw2bdqkZSEMTZ8r0oho+tyGRdPniog0Ugp0EZGYUKCLNDJRDbNKzRzM70mBLtKItGrVis2bNyvU6zl3Z/PmzbRq1apGz9NVLiKNSGZmJsXFxWzatCnqUqQarVq1IjMzs0bPUaCLNCLNmzena9euUZchtURDLiIiMaFAFxGJCQW6iEhMKNBFRGJCgS4iEhMKdBGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiotpAN7PbzOwjM1tdyeOHm9nDZvaqma0xs/PTX6aIiFQnlR76HcDYKh7/PvCGuw8ExgB/MLMWh16aiIjURLWB7u5LgU+qagK0NTMD2iTa7klPeSIikqp0jKHfAPQG3gdeB37o7vuSNTSzi8ys0MwKNcG+iEh6pSPQvwmsAo4DcoAbzKxdsobuPt/d89w9r2PHjml4aRERKZGOQD8feMCDt4H3gF5p2K+IiNRAOgJ9A3AKgJkdA/QE3k3DfkVEpAaqXVPUzO4hXL3SwcyKgZlAcwB3nwf8CrjDzF4HDLjK3T+utYpFRCSpagPd3SdX8/j7wDfSVpGIiBwUfVJURCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhITCnQRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMaFAFxGJCQW6iEhMKNBFRGJCgS4iEhMKdBGRmFCgi4jEhAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhITCnQRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYqLaQDez28zsIzNbXUWbMWa2yszWmNmz6S1RRERSkUoP/Q5gbGUPmtkRwI3AGe7eFzgrLZWJiEiNVBvo7r4U+KSKJt8FHnD3DYn2H6WpNhERqYF0jKH3AI40syVmttzMzq2soZldZGaFZla4adOmNLy0iIiUSEegNwMGA6cD3wSuM7MeyRq6+3x3z3P3vI4dO6bhpUVEpESzNOyjGPjY3T8HPjezpcBA4K007FtERFKUjh76/wInmlkzM8sAhgFvpmG/IiJSA9X20M3sHmAM0MHMioGZQHMAd5/n7m+a2ePAa8A+4BZ3r/QSRxERqR3VBrq7T06hze+B36elIhEROSj6pKiISEwo0EVEYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMdEgA33XrqgrEBGpfxpcoC9dCl27wt13g3vU1YiI1B8NLtAPPxwyMyE/H77+dVi3LuqKRETqhwYX6AMHwksvwZ/+BIWFMGAAXHcd7NwZdWUiItFqcIEO0LQpXHoprF0LZ50Fv/419OsHjz0WdWUiItFpkIFe4thj4a674KmnoHlzOO00mDQJioujrkxEpO416EAvcfLJ8Oqroaf+yCPQuzf88Y+wZ0/UlYmI1J1YBDpAy5Zw7bWwZg2MHg0/+hEMHgwvvhh1ZSIidSM2gV6iWzdYtAjuvx8++QRGjIALL4TNm6OuLBoFBZCdDU2ahO8FBVFXJCK1JXaBDmAGEyfCG2/AjBlw++3Qq1f43piuXS8ogIsugqKi8HMXFYX7CnWReIploJdo2xb+8AdYsQK6d4dp08JwzOpGskDetdfCjh0HbtuxI2wXkfiJdaCXGDAAli2DW24JvfbcXLjqKvj886grq10bNtRsu4g0bI0i0CGMIV9wQfhk6ZQp8Lvfhath/vd/o66s9mRl1Wy7iDRsjSbQS3ToALfdBs89F6YRmDABzjgD1q+PurL0mz0bMjIO3JaREbaLSPw0ukAvMWpUGFv/3e/CB5P69IHf/ha+/DLqytInPx/mz4cuXcKJ4i5dwv38/KgrE5HaYB7RZR95eXleWFgYyWuXt2ED/PCHsHBhGIa56Sb42teirkpEpCIzW+7ueckea7Q99LKysuDBB+Hhh8MkX2PGwHnnwUcfRV2ZiEjqFOhljB8fPmn605/CPfeEa9f//GfYty/qykREqqdALycjA37zmzA3zIABcMkl4dOmK1dGXZmISNUU6JXo3RueeQbuvBPefRfy8uDyy2Hr1qgrExFJToFeBbNwzfq6deEj83PnhqC/997GNYWAiDQMCvQUHHlkuPLlxRfhmGPg7LNh7Fh4++2oKxMRKaVAr4Fhw+Dll+F//ieEe79+8ItfwK5dUVcmIqJAr7FmzeCyy8LydxMmwKxZ4eTp3/8edWUi0tgp0A/SccfBggXwxBNhPP0b34BzzoH334+6MhFprBToh+gb34DXXw899YULw7Xrc+fC3r1RVyYijY0CPQ1atYKZM0Own3BCmEZgyJAw3i4iUlcU6GnUvTs8/jj87W/wwQcwfDhMnw6ffhp1ZSLSGCjQ08wM/uM/wknTyy4Lsxv26gV33aVr10WkdlUb6GZ2m5l9ZGZVLtxmZkPMbK+ZTUpfeQ1Xu3Zw/fVQWBgWZ54yBU4+Gd58M+rKRCSuUumh3wGMraqBmTUF/g/wRBpqipXcXHjhBZg3D1atgoED4ZprKq71KSJyqKoNdHdfCnxSTbMfAPcDmnA2iaZN4eKLwxQCkyfDf/839O0LjzwSdWUiEieHPIZuZp2B7wDzUmh7kZkVmlnhpk2bDvWlG5yjj4a//CVM+tW6dZiud+JE+Ne/oq5MROIgHSdFrweucvdqr7x29/nunufueR07dkzDSzdMY8aE4Zff/CZcFdO7N8yZA7t3R12ZiDRk6Qj0PGCBma0HJgE3mtmENOw31lq0CAtpvPEGnHQSXHklDBoEzz8fdWUi0lAdcqC7e1d3z3b3bOA+4FJ3X3io+20ssrPhoYfCEnhbtoTFqy+4AD7+OOrKRKShSeWyxXuAF4GeZlZsZheY2SVmdkntl9c4mIWJvt54I/TU77wzXLt+661a/k5EUmce0add8vLyvLCwMJLXru9Wrw6fMF22LCx/N28e9O8fdVUiUh+Y2XJ3z0v2mD4pWg/16wfPPgu33RYudczNhR//GLZvj7oyEanPFOj1VJMmcP75IdDPPx/+8IdwNcwDD2gKARFJToFez7VvDzffHK5+OeooOPPMcP36e+9FXZmI1DcK9AZixAhYvjz01J99Fvr0Cdexf/ll1JWJSH2hQG9AmjWDGTPCTI6nnw7XXhvmhnnmmagrE5H6QIHeAGVmwn33waOPwhdfhFkcv/c9+PDDqCsTkSgp0BuwceNgzRr42c/g3nuhZ0+46SYtfyfSWCnQG7jWreFXvwrL3w0eDJdeGpbBW7Ei6spEpK4p0GOiZ09YvBgKCmDDhrCm6WWXhekERKRxUKDHiBl897vhpOn06XDDDWEKgXvugT17oq5ORGqbAj2GjjgihPnLL0PnziHkO3YM3+++Gz6pbrkSEWmQFOgxlpcH//hH+HTpd74DTz0F+fkh3EePht/9Lqxxqk+eisSDJudqRPbtg1degUWLwteqVWF7t27h06fjx4egb9ky0jJFpAqanEuAMD/MsGHhqpiVK8PJ03nzwhwx8+fDN74BHTrApElwxx3wUSNcIbagIMxR36RJ+F5QEHVFIqlTD10A2LEDnn66tPf+73+Hk6xDh5b23gcODNviqqAALrooHIsSGRnhj11+fnR1iZRVVQ9dgS4VuIfhmJJwf/nlsD0zszTcTz45XAMfJ9nZUFRUcXuXLrB+fV1XI5KcAl0OyQcfwGOPhXB/8skwL3vr1nDKKaUB37lz1FUeuiZNkp8gNtPKUVJ/KNAlbb74Isz2uGgRPPxwac81N7c03PPyQjg2NOqhS0Ogk6KSNi1bhpOnc+fCu++GuWR++1s47DCYPTucdD3uuLDQ9YMPwrZtUVecutmzw5h5WRkZYbtIQ6AeuqTN5s3w+OOh9/7YY2HagRYtYMyY0t57165RV1m1goIwLfGGDZCVFcJcJ0SlPtGQi9S53bvDKkslJ1bXrQvb+/SBb30rhPvw4WGOdxFJnQJdIvfPf5aG+9KlYW6Zo44KUwCPHw9jx4YpC0Skagp0qVe2bAlXyyxaFBbp+PhjaNoUTjyxdGimR494X/MucrAU6FJv7d0b5psp6b2//nrY/tWvhmD/1rdg1KgwFi8iCnRpQIqK4JFHwiWRTz8dFsFu1w6++c0Q8OPGhcnFRBorBbo0SNu3hxkiS3rvH3wQhmFOOKF0aKZfPw3NSOOiQJcGb9++sKxeSbgvXx62Z2WVhvtJJ0GrVtHWKVLbFOgSO++/H06oLloEf/97mFArIwO+/vUQ7qefDp06RV2lSPop0CXWdu6EJUtKpyP417/C9sGDS0+s5uY2zOkIRMpToEuj4Q6rV4dgX7QIXnopbOvUKfTax4+HU08NUxWINEQKdGm0Nm0qnSny8cfD3DItW4bpf0uGZrp0ibpKkdQp0EUIl0A+91zp0Mw774Tt/fuXnlgdNix8yEmkvlKgi5TjDm+9VRruy5aFDzl16BCudR82DAYMCGGvKQmkPlGgi1Tj00/hiSdCwD/xRJiOoERWVgj3sl/du2tiMYmGAl2kBtzDZZGvvXbg19q1YVIxCOPwffuGdVbLBn2HDtHWXtc03XDdU6CLpMEXX4RQLwn4V18N3z/8sLRNp04Ve/O9esVzLhotqh2NQwp0M7sNGA985O79kjyeD1yVuLsdmO7ur1ZXlAJd4uLDD8OkYmV782vWhJOwEIZmeveuGPSdOjXsaQu0ZF80DjXQRxOC+s5KAn0E8Ka7f2pm44BZ7j6suqIU6BJnu3eHOeDLD9uUfOgJoH37iiHft29YgLsh0KLa0agq0Ks9rePuS80su4rHXyhz9yUgs8YVisRM8+ZhdaY+feCcc0q3f/ppxd78zTeXDls0aRJOuJYfm8/Kqn+9+ays5D30rKy6r0WCdJ+nvwB4LM37FImNI4+E0aPDV4m9e8OC22VDvrAQ7r23tE27dhV78/36Qdu2df8zlJg9O/kYuhbVjk7aAt3MTiIE+qgq2lwEXASQpT/jIkD4IFP37uHrzDNLt2/bFqYxKBv0d90FW7eWtunWrWLQf+UrdTNvTcmJT13lUn+kdJVLYshlUbIx9MTjA4AHgXHu/lYqL6wxdJGacw/hWX5s/q23SsetMzJC733AgNKhm/79w7sDafgOaQw9hZ1nAQ8AU1INcxE5OGbhKpIuXcIskiV27oQ33jjwksoHH4Rbbiltc/zxFXvzPXroA1JxUu2v0szuAcYAHcysGJgJNAdw93nAz4H2wI0Wztrsqeyvh4jUjtatw3TBgweXbnOHjRsr9uafeOLAD0j16VMx6I8+OpqfQw6NPlgk0sh8+eWBH5Aq+dq4sbTNMcdUDPnevcMfAIlWrQ65iEjD0qJFaUiXtWlTxUsqb7ghfEIWwtBMz54VL6k87rj6d0llY6UeuohUas8eePvtitMdbNhQ2uaoo8KVNVlZ4atLlwO/t2+vwE8nzeUiImn12WelvfnXX4f33gshX1QUTtCWlZFxYMCXD/3OncMHsSQ1GnIRkbQ64gg48cTwVZY7bN5cGu7lv69cCR99dOBzmjQJwzZVhX67dnX2ozVoCnQRSRuzMIVwhw4waFDyNjt3hjltkoX+P/4B990X5sIp6/DDKw/7rKww0ZkWAVegi0gda906XP/eo0fyx/ftCzNYJuvhb9gAzz8f5sQpq3lzyMysPPSPPz4M/cSdAl1E6pUmTUKPu1MnGD48eZutW0Mvv3zYb9gAzzwD//53xRkfO3asvIffpUt4V9HQT94q0EWkwWnXLkw13Ldv8sf37AmhnqyHv24dPPkkfP75gc9p1arysM/KCu8ADnWhktpe4UmBLiKx06xZ6RQJ5U/cQjh5++mnyXv4RUXwyCPwwQcHPscsvGuoKvSrWlC8/ApPRUXhPqQv1HXZoohIErt2QXFxxbAv+71kVaoS7dpVPqxz9tnhXUN5NV3hSZctiojUUKtW8NWvhq9k9u0Ll2BWFvYvvQSffFL965T9kNahUqCLiByEJk3g2GPD19Chydts314a+Pn5yQM+nUtD6MpNEZFa0qZNmM1y7FiYO7fipZPpXuFJgS4iUgfy82H+/DBmXjKv/fz5uspFRKRBys+v3SX61EMXEYkJBbqISEwo0EVEYkKBLiISEwp0EZGYUKCLiMREZHO5mNkmoOggn94B+DiN5aRLfa0L6m9tqqtmVFfNxLGuLu7eMdkDkQX6oTCzwsomp4lSfa0L6m9tqqtmVFfNNLa6NOQiIhITCnQRkZhoqIE+P+oCKlFf64L6W5vqqhnVVTONqq4GOYYuIiIVNdQeuoiIlKNAFxGJiXod6GY21szWmdnbZnZ1ksfNzOYmHn/NzAbVk7rGmNkWM1uV+Pp5HdV1m5l9ZGarK3k8quNVXV11frzM7Hgze8bM3jSzNWb2wyRt6vx4pVhXFMerlZm9bGavJur6RZI2URyvVOqK5P9j4rWbmtlKM1uU5LH0Hy93r5dfQFPgHaAb0AJ4FehTrs1pwGOAAcOBf9STusYAiyI4ZqOBQcDqSh6v8+OVYl11fryATsCgxO22wFv15N9XKnVFcbwMaJO43Rz4BzC8HhyvVOqK5P9j4rVnAHcne/3aOF71uYc+FHjb3d919y+BBcC3y7X5NnCnBy8BR5hZp3pQVyTcfSlQ1bK0URyvVOqqc+6+0d1XJG5vA94EOpdrVufHK8W66lziGGxP3G2e+Cp/RUUUxyuVuiJhZpnA6cAtlTRJ+/Gqz4HeGfhXmfvFVPyHnUqbKOoCOCHxNvAxM+tbyzWlKorjlarIjpeZZQO5hN5dWZEeryrqggiOV2L4YBXwEfB3d68XxyuFuiCaf1/XAz8B9lXyeNqPV30OdEuyrfxf3lTapFsqr7mCMN/CQOD/AgtruaZURXG8UhHZ8TKzNsD9wOXuvrX8w0meUifHq5q6Ijle7r7X3XOATGComfUr1ySS45VCXXV+vMxsPPCRuy+vqlmSbYd0vOpzoBcDx5e5nwm8fxBt6rwud99a8jbQ3R8FmptZh1quKxVRHK9qRXW8zKw5ITQL3P2BJE0iOV7V1RX1vy93/wxYAowt91Ck/74qqyui4zUSOMPM1hOGZU82s7vKtUn78arPgf4K0N3MuppZC+Ac4KFybR4Czk2cLR4ObHH3jVHXZWbHmpklbg8lHOfNtVxXKqI4XtWK4nglXu9W4E13/2Mlzer8eKVSV0THq6OZHZG43Ro4FVhbrlkUx6vauqI4Xu7+U3fPdPdsQkY87e7fK9cs7cer2aE8uTa5+x4z+y/gCcKVJbe5+xozuyTx+DzgUcKZ4reBHcD59aSuScB0M9sD7ATO8cRp7dpkZvcQzuh3MLNiYCbhJFFkxyvFuqI4XiOBKcDrifFXgGuArDJ1RXG8UqkriuPVCfiLmTUlBOK97r4o6v+PKdYVyf/HZGr7eOmj/yIiMVGfh1xERKQGFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhITCnQRkZj4/zqx8dVQFdYXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = cnn_train.history['acc']\n",
    "val_accuracy = cnn_train.history['val_acc']\n",
    "loss = cnn_train.history['loss']\n",
    "val_loss = cnn_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viii. Dataframe with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.787309</td>\n",
       "      <td>0.783472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression - Oversampled</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.785650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.752615</td>\n",
       "      <td>0.752196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XG Boost Model</td>\n",
       "      <td>0.745374</td>\n",
       "      <td>0.737790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.741251</td>\n",
       "      <td>0.736116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression - Undersampled</td>\n",
       "      <td>0.705953</td>\n",
       "      <td>0.720983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.685438</td>\n",
       "      <td>0.671891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.570634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model  Accuracy Score  F1 Score\n",
       "0                 Logistic Regression        0.787309  0.783472\n",
       "3   Logistic Regression - Oversampled        0.784493  0.785650\n",
       "1             Support Vector Machines        0.752615  0.752196\n",
       "2                      XG Boost Model        0.745374  0.737790\n",
       "5                         Naive Bayes        0.741251  0.736116\n",
       "4  Logistic Regression - Undersampled        0.705953  0.720983\n",
       "7                                 CNN        0.685438  0.671891\n",
       "6                       Random Forest        0.628821  0.570634"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Support Vector Machines', 'XG Boost Model', 'Logistic Regression - Oversampled', \n",
    "              'Logistic Regression - Undersampled', 'Naive Bayes', 'Random Forest', 'CNN'],\n",
    "    'Accuracy Score': [acc_lr, acc_svm, acc_xgb, acc_os_lr, acc_us_lr, acc_mnb, acc_rf, accuracy],\n",
    "    'F1 Score': [f1_lr, f1_svm, f1_xgb, f1_os_lr, f1_us_lr, f1_mnb,  f1_rf, f1_score]})\n",
    "\n",
    "models.sort_values(by = 'Accuracy Score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Fitting Best Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Saving & Loading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pkl\n",
    "def load_pkl(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as pkl_file:\n",
    "        return pickle.load(pkl_file)\n",
    "    \n",
    "#Save pkl\n",
    "def save_pkl(file, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as pkl_file:\n",
    "        pickle.dump(file, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "#using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cuisine_predictions():\n",
    "    classifier = LogisticRegression(random_state=123,\n",
    "                                      max_iter=300,\n",
    "                                      n_jobs=-1,\n",
    "                                      multi_class = 'multinomial', \n",
    "                                      solver = 'newton-cg', \n",
    "                                      C = 10, \n",
    "                                      penalty = 'l2') \n",
    "    #Pipeline\n",
    "    model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                                (\"classifier\", classifier)])\n",
    " \n",
    "\n",
    "    #Fit Model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_prob = model.predict_proba(X_train)\n",
    " \n",
    "    #Save as Pkl\n",
    "    save_pkl(model, os.path.join(model_path, \"pickle_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_cuisine_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Creating a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/nlp'\n",
    "\n",
    "def create_and_populate_db():\n",
    "    data = import_recipe_data()\n",
    "    \n",
    "    #Process the data\n",
    "    data = process_recipes(data)\n",
    "    \n",
    "    #Predict cuisine from trained model\n",
    "    model = load_pkl(os.path.join(model_path, 'pickle_model.pkl'))\n",
    "    data[\"cuisine\"] = model.predict(data[\"ingredients_query\"].tolist())  #this is predicting cuisine for recipe dataset\n",
    "    \n",
    "    db = sq.connect('recipes.db')\n",
    "    #Verify dtypes\n",
    "    for col in data.columns:\n",
    "        data[col] = data[col].astype('str')\n",
    "\n",
    "    print(' ------------------ Check data before populating the db ------------------')\n",
    "    print(data.columns)\n",
    "    print(data.head())\n",
    "    print(data.shape)\n",
    "    data.to_sql('main_recipes', db, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------------ Check data before populating the db ------------------\n",
      "Index(['title', 'ingredients', 'instructions', 'ingredients_query', 'cuisine'], dtype='object')\n",
      "                               title  \\\n",
      "0  Slow Cooker Chicken and Dumplings   \n",
      "1      Awesome Slow Cooker Pot Roast   \n",
      "2               Brown Sugar Meatloaf   \n",
      "3        Best Chocolate Chip Cookies   \n",
      "4  Homemade Mac and Cheese Casserole   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  ['4 skinless, boneless chicken breast halves A...   \n",
      "1  ['2 (10.75 ounce) cans condensed cream of mush...   \n",
      "2  ['1/2 cup packed brown sugar ADVERTISEMENT', '...   \n",
      "3  ['1 cup butter, softened ADVERTISEMENT', '1 cu...   \n",
      "4  ['8 ounces whole wheat rotini pasta ADVERTISEM...   \n",
      "\n",
      "                                        instructions  \\\n",
      "0  Place the chicken, butter, soup, and onion in ...   \n",
      "1  In a slow cooker, mix cream of mushroom soup, ...   \n",
      "2  Preheat oven to 350 degrees F (175 degrees C)....   \n",
      "3  Preheat oven to 350 degrees F (175 degrees C)....   \n",
      "4  Preheat oven to 350 degrees F. Line a 2-quart ...   \n",
      "\n",
      "                                   ingredients_query      cuisine  \n",
      "0   skinless boneless chicken breast half butter ...  southern_us  \n",
      "1   can condensed cream mushroom soup package dry...  southern_us  \n",
      "2   packed brown sugar ketchup lean ground beef m...  southern_us  \n",
      "3   butter softened white sugar packed brown suga...      italian  \n",
      "4   whole wheat rotini pasta fresh broccoli flore...      italian  \n",
      "(124647, 5)\n"
     ]
    }
   ],
   "source": [
    "create_and_populate_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_db(cuisine):\n",
    "    db = sq.connect('recipes.db')\n",
    "    sql_query = \"SELECT title, instructions, ingredients, ingredients_query FROM main_recipes WHERE cuisine = ?\"\n",
    "    return pd.read_sql(sql_query, db, params = (cuisine, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Doc2Vec function for recipe similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_embeddings(data): #Recipe dataset\n",
    "    data = data['ingredients_query'].tolist()\n",
    "    tagged_data = [TaggedDocument(words=row.split(), tags=[str(index)]) for index, row in enumerate(data)]\n",
    "\n",
    "    max_epochs = 50\n",
    "    vec_size = 200\n",
    "    alpha = 0.025\n",
    "\n",
    "    model_embedding = Doc2Vec(vector_size=vec_size,\n",
    "                        alpha=alpha, \n",
    "                        min_alpha=0.00025,\n",
    "                        min_count=3,\n",
    "                        dm =0, # Paragraph Vector - Distributed Bag of Words\n",
    "                        window=5)\n",
    "  \n",
    "    model_embedding.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model_embedding.train(tagged_data,\n",
    "                    total_examples=model_embedding.corpus_count,\n",
    "                    epochs=model_embedding.epochs)\n",
    "        # decrease the learning rate\n",
    "        model_embedding.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model_embedding.min_alpha = model_embedding.alpha\n",
    "        # vector size\n",
    "        model_embedding.vector_size /= 2\n",
    "        # window\n",
    "        model_embedding.window -= 2\n",
    " \n",
    "    return model_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_embeddings():\n",
    "    db = sq.connect('recipes.db')\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    for cuisine in cuisine_classes:\n",
    "        sql_query = f\"SELECT title, instructions, ingredients, ingredients_query FROM main_recipes WHERE cuisine = ?\"\n",
    "        data = pd.read_sql(sql_query, db, params=(cuisine,))\n",
    "        \n",
    "        model_embedding = d2v_embeddings(data)\n",
    "        save_pkl(model_embedding, os.path.join(model_embeddings_path, f'd2v_{cuisine}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H. Final functions for Chatbot construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Predict Cuisine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cuisine(input_text):\n",
    "    top = 3\n",
    "\n",
    "    #Tokenize text\n",
    "    tokenize_text = get_tokenize_text(input_text)\n",
    "\n",
    "    #Get model\n",
    "    model_def = os.path.join(model_path, 'pickle_model.pkl')\n",
    "    model = load_pkl(model_def)\n",
    "\n",
    "    #Tokenize text\n",
    "    tokenize_text = get_tokenize_text(input_text)\n",
    " \n",
    "    #Get classes ordered by probability\n",
    "    proba = model.predict_proba([tokenize_text])[0]\n",
    " \n",
    "    #Sorted index list \n",
    "    indexes = sorted(range(len(proba)), key = lambda k: proba[k], reverse=True)\n",
    " \n",
    "    #Get cuisine\n",
    "    cuisine_labels = model.classes_.tolist()\n",
    "    cusine_ordered = [cuisine_labels[ind] for ind in indexes]\n",
    " \n",
    "    return cusine_ordered[: top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Getting similar recipes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_recipes(input_text, cuisine, top_k = 3):\n",
    "    #Tokenize text\n",
    "    tokenize_text = get_tokenize_text(input_text).split()\n",
    "    \n",
    "    #Load model from the selected cuisine\n",
    "    d2v = load_pkl(os.path.join(model_embeddings_path, f'd2v_{cuisine}.pkl'))\n",
    "\n",
    "    #Get embeddings\n",
    "    embeddings = d2v.infer_vector(tokenize_text)\n",
    "    best_recipes = d2v.docvecs.most_similar([embeddings]) #gives you top 10 document tags and their cosine similarity\n",
    "\n",
    "    #Get recipes\n",
    "    best_recipes_index = [int(output[0]) for output in best_recipes]\n",
    "    \n",
    "    #Get dDtaFrame\n",
    "    df = get_df_from_db(cuisine)\n",
    "    \n",
    "    return df[df.index.isin(best_recipes_index)].head(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Telegram Chatbot Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Defining the initial arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the parser\n",
    "my_parser = argparse.ArgumentParser(description = 'Give your personal token')\n",
    "\n",
    "#Add the arguments\n",
    "my_parser.add_argument('token', metavar = 'token', type = str, help = 'The token given by Fatherbot')\n",
    "\n",
    "\n",
    "#Enable logging\n",
    "logging.basicConfig(\n",
    "    #filename= 'telgramBot.log',\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Main interactions ----\n",
    "CHOOSING, GET_TEXT = range(2)\n",
    "#Callback data\n",
    "CALLBACK1, CALLBACK2 = range(3, 5)\n",
    "\n",
    "reply_keyboard = [\n",
    "    ['Show ingredients', 'Get recipes'],\n",
    "    ['Remove item', 'Done'],\n",
    "]\n",
    "markup = ReplyKeyboardMarkup(reply_keyboard, one_time_keyboard=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Functions for the chatbot flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: Start\")\n",
    "\n",
    "    context.user_data['chat_id'] = update.message.chat_id\n",
    "\n",
    "    update.message.reply_text(\n",
    "        \"Hi! I am your recipe bot. What ingredients do you currently have? \"\n",
    "        \"You can add ingredients by typing it in one or two words\",\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def get_basket_txt(list_ingredients):\n",
    "    txt = 'Here are your current ingredients:\\n'\n",
    "    for ingredient in list_ingredients:\n",
    "        txt += f'   - {ingredient}\\n'\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def button1(update: Update, context: CallbackContext) -> int:\n",
    "    logger.info(f\": button1\")\n",
    "\n",
    "    query = update.callback_query\n",
    "    query.answer()\n",
    "\n",
    "    user_data = context.user_data    \n",
    "    if 'ingredients_list' not in user_data:\n",
    "        user_data['ingredients_list'] = [query.data]\n",
    "    else:\n",
    "        user_data['ingredients_list'].append(query.data)\n",
    "\n",
    "    query.edit_message_text(text=f\"Ok you selected: {query.data}\")\n",
    "    \n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    context.bot.send_message(chat_id=context.user_data['chat_id'], text=txt)\n",
    "\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def recipes_query(update: Update, context: CallbackContext) -> int:\n",
    "    \"\"\" Get recipes \"\"\"\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: recipes_query\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "\n",
    "    input_text = ' '.join(user_data['ingredients_list'])\n",
    "\n",
    "    # Predict cuisine\n",
    "    cuisine = predict_cuisine(input_text)\n",
    "\n",
    "    keyboard = [\n",
    "        [\n",
    "            InlineKeyboardButton(cuisine[0], callback_data=cuisine[0]),\n",
    "            InlineKeyboardButton(cuisine[1], callback_data=cuisine[1]),\n",
    "            InlineKeyboardButton(cuisine[2], callback_data=cuisine[2])]\n",
    "    ]\n",
    "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
    "    # Send message with text and appended InlineKeyboard\n",
    "    update.message.reply_text(\"Chose the type of cuisine you want!\", reply_markup=reply_markup)\n",
    "\n",
    "    return CALLBACK2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def button2(update: Update, context: CallbackContext) -> int:\n",
    "    #user = update.message.from_user\n",
    "    logger.info(f\"button2\")\n",
    "\n",
    "    query = update.callback_query\n",
    "    query.answer()\n",
    "\n",
    "    # Get recipes\n",
    "    recipes = get_similar_recipes(context.user_data['ingredients_list'], query.data)\n",
    "\n",
    "    sep = '\\n\\n'\n",
    "    for index, row in recipes.iterrows():\n",
    "\n",
    "        title = 'Title: ' + row['title'] \n",
    "        ingredients=''\n",
    "        list_ing = row['ingredients'].replace('ADVERTISEMENT', '').strip('][').split(', ')\n",
    "        for ingredient in list_ing:\n",
    "            ingredients+= ingredient.replace(\"'\", \"\") + '\\n'\n",
    "        ingredients = 'Ingredients: ' + '\\n' + ingredients\n",
    "        instructions = 'Instruction: '+ '\\n' + row['instructions']\n",
    "\n",
    "        txt = title + sep + ingredients + sep + instructions\n",
    "\n",
    "        context.bot.send_message(chat_id=context.user_data['chat_id'], text=txt)\n",
    "\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def show_basket(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: show_basket\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    \n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    \n",
    "    update.message.reply_text(\n",
    "        txt,\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def received_text_information(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: received_text_information\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    text = update.message.text\n",
    "    \n",
    "    if 'ingredients_list' not in user_data:\n",
    "        user_data['ingredients_list'] = [text]\n",
    "    else:\n",
    "        user_data['ingredients_list'].append(text)\n",
    "\n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    update.message.reply_text(\n",
    "        txt,\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def remove_item(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: remove_item\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    if 'ingredients_list' in user_data:\n",
    "        del user_data['ingredients_list'][-1]\n",
    "    \n",
    "    introduction = 'You have deleted the last ingredient. '\n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    update.message.reply_text(\n",
    "        introduction + txt,\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def done(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: done\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    if 'ingredients_list' in user_data:\n",
    "        del user_data['ingredients_list']\n",
    "\n",
    "    update.message.reply_text(\n",
    "        f\"Goodbye for now!\"\n",
    "    )\n",
    "\n",
    "    user_data.clear()\n",
    "    return ConversationHandler.END    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(bot_token) -> None:\n",
    "    updater = Updater(bot_token, use_context=True)\n",
    "\n",
    "    # Get the dispatcher to register handlers\n",
    "    dispatcher = updater.dispatcher\n",
    "\n",
    "    # Add conversation handler with the states CHOOSING, TYPING_CHOICE and TYPING_REPLY\n",
    "    conv_handler = ConversationHandler(\n",
    "        entry_points=[CommandHandler('start', start)],\n",
    "        states={\n",
    "            CHOOSING: [\n",
    "                MessageHandler(Filters.text & ~(Filters.command | Filters.regex('^(Done|Get recipes|Show ingredients|Remove item)$')), received_text_information),\n",
    "                MessageHandler(Filters.regex('^Get recipes$'), recipes_query),\n",
    "                MessageHandler(Filters.regex('^Show ingredients$'), show_basket),\n",
    "                MessageHandler(Filters.regex('^Remove item$'), remove_item),\n",
    "            ],\n",
    "            CALLBACK1: [\n",
    "                CallbackQueryHandler(button1)],\n",
    "            CALLBACK2: [\n",
    "                CallbackQueryHandler(button2)],\n",
    "        },\n",
    "        fallbacks=[MessageHandler(Filters.regex('^Done$'), done)],\n",
    "        per_message=False,\n",
    "    )\n",
    "\n",
    "    dispatcher.add_handler(conv_handler)\n",
    "    \n",
    "    updater.start_polling()\n",
    "    updater.idle()\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        args = my_parser.parse_args()\n",
    "        main(args.token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Demonstrate using Bot token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 15:16:01,052 - apscheduler.scheduler - INFO - Scheduler started\n",
      "2022-05-02 15:16:19,975 - __main__ - INFO - Reina: Start\n",
      "2022-05-02 15:16:28,792 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:16:30,477 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:16:32,167 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:16:34,047 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:16:38,354 - __main__ - INFO - Reina: remove_item\n",
      "2022-05-02 15:16:42,813 - __main__ - INFO - Reina: recipes_query\n",
      "2022-05-02 15:16:48,615 - __main__ - INFO - button2\n",
      "2022-05-02 15:17:05,973 - __main__ - INFO - Reina: done\n",
      "2022-05-02 15:29:20,345 - __main__ - INFO - Reina: Start\n",
      "2022-05-02 15:29:28,294 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:29:30,695 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:29:33,737 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:29:35,774 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:29:40,616 - __main__ - INFO - Reina: remove_item\n",
      "2022-05-02 15:29:45,127 - __main__ - INFO - Reina: recipes_query\n",
      "2022-05-02 15:29:53,429 - __main__ - INFO - button2\n",
      "2022-05-02 15:30:08,702 - __main__ - INFO - Reina: done\n",
      "2022-05-02 15:34:04,973 - __main__ - INFO - Reina: Start\n",
      "2022-05-02 15:34:13,775 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:34:16,767 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:34:19,028 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:34:22,159 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:34:27,924 - __main__ - INFO - Reina: remove_item\n",
      "2022-05-02 15:34:34,427 - __main__ - INFO - Reina: recipes_query\n",
      "2022-05-02 15:34:42,026 - __main__ - INFO - button2\n",
      "2022-05-02 15:35:00,930 - __main__ - INFO - Reina: done\n",
      "2022-05-02 15:38:23,375 - __main__ - INFO - Reina: Start\n",
      "2022-05-02 15:38:33,403 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:38:36,499 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:38:39,784 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:38:43,103 - __main__ - INFO - Reina: received_text_information\n",
      "2022-05-02 15:38:48,393 - __main__ - INFO - Reina: remove_item\n",
      "2022-05-02 15:38:56,377 - __main__ - INFO - Reina: recipes_query\n",
      "2022-05-02 15:39:09,569 - __main__ - INFO - button2\n",
      "2022-05-02 15:39:30,751 - __main__ - INFO - Reina: done\n",
      "2022-05-02 15:40:37,187 - telegram.ext.updater - INFO - Received signal 2 (SIGINT), stopping...\n",
      "2022-05-02 15:40:37,188 - apscheduler.scheduler - INFO - Scheduler has been shut down\n",
      "usage: ipykernel_launcher.py [-h] token\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#Need to have telegram and Botfather enabled to access the same!\n",
    "main('5210893580:AAGzUeZ-nSp3d2YiG-G1A2vlRc_ohpEV9qk') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Link from Kaggle: https://www.kaggle.com/code/rahulsridhar2811/cuisine-classification-with-accuracy-78-88/notebook\n",
    "#### Additional:\n",
    "\n",
    "* https://htmlpreview.github.io/?https://github.com/kulsoom-abdullah/kulsoom-abdullah.github.io/blob/master/AWS-lambda-implementation/model_implementation/recipe_binary_classification/recipe%20binary%20classification.html#Naive-Bayes\n",
    "* https://htmlpreview.github.io/?https://github.com/kulsoom-abdullah/kulsoom-abdullah.github.io/blob/master/AWS-lambda-implementation/model_implementation/recipe_multiclass_classification/recipe%20multiclass%20classification.html#Logistic-Regression\n",
    "* https://github.com/kulsoom-abdullah/kulsoom-abdullah.github.io/tree/master/AWS-lambda-implementation/model_implementation/recipe_multiclass_classification  \n",
    "\n",
    "\n",
    "* https://github.com/RomainGratier/recipes-telegram-bot\n",
    "* https://towardsdatascience.com/a-recommendation-engine-that-proposes-recipes-after-taking-photos-of-your-ingredients-de2d314f565d?source=friends_link&sk=c5280f8c50aa5551d1b36619891e9b4f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db968608ecdb27000edf98c5f63e9ccc25d479a7a16895421f990979c50088b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
