{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8562ce4a",
   "metadata": {},
   "source": [
    "recipes-telegram-bot/src/recommendation_engine/data.py /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dcdd6",
   "metadata": {},
   "source": [
    "1. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e49343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_CUISINE_PATH = \"data/cuisine_data/\"\n",
    "DATA_RECIPES_PATH = \"data/recipes_data/\"\n",
    "\n",
    "def import_data():\n",
    "    train = pd.read_json(os.path.join(DATA_CUISINE_PATH, 'train.json'))\n",
    "    test = pd.read_json(os.path.join(DATA_CUISINE_PATH, 'test.json'))\n",
    "    return pd.concat([train,test],axis=0)\n",
    "\n",
    "def import_recipes_main():\n",
    "    data_path_ar = os.path.join(DATA_RECIPES_PATH, \"recipes_raw_nosource_ar.json\")\n",
    "    data_path_epi = os.path.join(DATA_RECIPES_PATH, \"recipes_raw_nosource_epi.json\")\n",
    "    data_path_fn = os.path.join(DATA_RECIPES_PATH, \"recipes_raw_nosource_fn.json\")\n",
    "    \n",
    "    data =  pd.concat([pd.read_json(data_path_ar, orient='index'), pd.read_json(data_path_epi, orient='index'), pd.read_json(data_path_fn, orient='index')])\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(columns=['picture_link', 'index'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b64780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_recipes_main_test():\n",
    "    all_recipes = pd.read_json('./data/recipes_data/recipes_raw_nosource_allrecipes.json', orient='index')\n",
    "    epicurious = pd.read_json('./data/recipes_data/recipes_raw_nosource_epicurious.json', orient='index')\n",
    "    food_network = pd.read_json('./data/recipes_data/recipes_raw_nosource_foodnetwork.json', orient='index')\n",
    "    recipes = pd.concat([all_recipes, epicurious, food_network], axis=0)\n",
    "    \n",
    "    recipes = recipes.reset_index()\n",
    "    recipes = recipes.drop(columns=['index', 'picture_link'])\n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779d11a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slow Cooker Chicken and Dumplings</td>\n",
       "      <td>[4 skinless, boneless chicken breast halves AD...</td>\n",
       "      <td>Place the chicken, butter, soup, and onion in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome Slow Cooker Pot Roast</td>\n",
       "      <td>[2 (10.75 ounce) cans condensed cream of mushr...</td>\n",
       "      <td>In a slow cooker, mix cream of mushroom soup, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Sugar Meatloaf</td>\n",
       "      <td>[1/2 cup packed brown sugar ADVERTISEMENT, 1/2...</td>\n",
       "      <td>Preheat oven to 350 degrees F (175 degrees C)....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Chocolate Chip Cookies</td>\n",
       "      <td>[1 cup butter, softened ADVERTISEMENT, 1 cup w...</td>\n",
       "      <td>Preheat oven to 350 degrees F (175 degrees C)....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homemade Mac and Cheese Casserole</td>\n",
       "      <td>[8 ounces whole wheat rotini pasta ADVERTISEME...</td>\n",
       "      <td>Preheat oven to 350 degrees F. Line a 2-quart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124642</th>\n",
       "      <td>Summer Corn Salad</td>\n",
       "      <td>[4 ears fresh corn, 2 heads Belgian endive, 2 ...</td>\n",
       "      <td>Watch how to make this recipe.\\nPreheat a gril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124643</th>\n",
       "      <td>Zucchini Stuffed Tomatoes</td>\n",
       "      <td>[4 large plum tomatoes, Salt and sugar, 1 1/2 ...</td>\n",
       "      <td>Preheat the broiler. Cut the tomatoes in 1/2 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124644</th>\n",
       "      <td>Pepper Pasta Quick Cook</td>\n",
       "      <td>[3 tablespoons olive oil, 2 tablespoons unsalt...</td>\n",
       "      <td>Heat the oil and butter in a large skillet ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124645</th>\n",
       "      <td>Chocolate Cake with Armagnac Ice Cream</td>\n",
       "      <td>[8 ounces butter, 8 ounces bittersweet chocola...</td>\n",
       "      <td>Preheat oven to 350 degrees. On the top half o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124646</th>\n",
       "      <td>Crabby Bisque</td>\n",
       "      <td>[3 (10.5-ounce) cans restaurant-style condense...</td>\n",
       "      <td>Watch how to make this recipe.\\nIn a medium sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124647 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0            Slow Cooker Chicken and Dumplings   \n",
       "1                Awesome Slow Cooker Pot Roast   \n",
       "2                         Brown Sugar Meatloaf   \n",
       "3                  Best Chocolate Chip Cookies   \n",
       "4            Homemade Mac and Cheese Casserole   \n",
       "...                                        ...   \n",
       "124642                       Summer Corn Salad   \n",
       "124643               Zucchini Stuffed Tomatoes   \n",
       "124644                 Pepper Pasta Quick Cook   \n",
       "124645  Chocolate Cake with Armagnac Ice Cream   \n",
       "124646                           Crabby Bisque   \n",
       "\n",
       "                                              ingredients  \\\n",
       "0       [4 skinless, boneless chicken breast halves AD...   \n",
       "1       [2 (10.75 ounce) cans condensed cream of mushr...   \n",
       "2       [1/2 cup packed brown sugar ADVERTISEMENT, 1/2...   \n",
       "3       [1 cup butter, softened ADVERTISEMENT, 1 cup w...   \n",
       "4       [8 ounces whole wheat rotini pasta ADVERTISEME...   \n",
       "...                                                   ...   \n",
       "124642  [4 ears fresh corn, 2 heads Belgian endive, 2 ...   \n",
       "124643  [4 large plum tomatoes, Salt and sugar, 1 1/2 ...   \n",
       "124644  [3 tablespoons olive oil, 2 tablespoons unsalt...   \n",
       "124645  [8 ounces butter, 8 ounces bittersweet chocola...   \n",
       "124646  [3 (10.5-ounce) cans restaurant-style condense...   \n",
       "\n",
       "                                             instructions  \n",
       "0       Place the chicken, butter, soup, and onion in ...  \n",
       "1       In a slow cooker, mix cream of mushroom soup, ...  \n",
       "2       Preheat oven to 350 degrees F (175 degrees C)....  \n",
       "3       Preheat oven to 350 degrees F (175 degrees C)....  \n",
       "4       Preheat oven to 350 degrees F. Line a 2-quart ...  \n",
       "...                                                   ...  \n",
       "124642  Watch how to make this recipe.\\nPreheat a gril...  \n",
       "124643  Preheat the broiler. Cut the tomatoes in 1/2 c...  \n",
       "124644  Heat the oil and butter in a large skillet ove...  \n",
       "124645  Preheat oven to 350 degrees. On the top half o...  \n",
       "124646  Watch how to make this recipe.\\nIn a medium sa...  \n",
       "\n",
       "[124647 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_recipes_main_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240c814",
   "metadata": {},
   "source": [
    "recipes-telegram-bot/src/recommendation_engine/feature_engineering.py /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0c03c",
   "metadata": {},
   "source": [
    "2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0fb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, model_selection, pipeline, manifold, preprocessing\n",
    "\n",
    "#from src.recommendation_engine.data import import_data\n",
    "\n",
    "additional_stop_words = [\"advertisement\", \"advertisements\",\n",
    "                         \"cup\", \"cups\",\n",
    "                         \"tablespoon\", \"tablespoons\", \n",
    "                         \"teaspoon\", \"teaspoons\", \n",
    "                         \"ounce\", \"ounces\",\n",
    "                         \"salt\", \n",
    "                         \"pepper\", \n",
    "                         \"pound\", \"pounds\",\n",
    "                         ]\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e31b40",
   "metadata": {},
   "source": [
    "3. Pre-Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfeab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "\n",
    "    ## Remove digits\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    ## remove mutliple space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f095f1",
   "metadata": {},
   "source": [
    "4. Pre-Process Data: Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242a884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    dataset = import_data() # cuisine\n",
    "\n",
    "    def processing(row):\n",
    "        ls = row['ingredients']\n",
    "        return ' '.join(ls)\n",
    "\n",
    "    dataset['ingredients'] = dataset.apply(lambda x: processing(x), axis=1)\n",
    "    dataset.dropna(inplace=True)\n",
    "    dataset = dataset.drop(columns=['id']).reset_index(drop=True)\n",
    "\n",
    "    stop_word_list = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # Extend list of stop words\n",
    "    stop_word_list.extend(additional_stop_words)\n",
    "\n",
    "    dataset[\"ingredients_query\"] = dataset[\"ingredients\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=stop_word_list))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289238a",
   "metadata": {},
   "source": [
    "5. Create Embeddings from Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a519bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(dataset):\n",
    "    ## Tf-Idf (advanced variant of BoW)\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "    corpus = dataset[\"ingredients_query\"]\n",
    "    vectorizer.fit(corpus)\n",
    "    embedded_ingredients = vectorizer.transform(corpus)\n",
    "    dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "    ## Chi squarred correlation embeddings reduction\n",
    "    labels = dataset[\"cuisine\"]\n",
    "    names = vectorizer.get_feature_names()\n",
    "    p_value_limit = 0.95\n",
    "    dtf_features = pd.DataFrame()\n",
    "\n",
    "    for cat in np.unique(labels):\n",
    "        chi2, p = feature_selection.chi2(embedded_ingredients, labels==cat)\n",
    "        dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                       {\"feature\":names, \"score\":1-p, \"labels\":cat}))\n",
    "        dtf_features = dtf_features.sort_values([\"labels\",\"score\"], \n",
    "                        ascending=[True,False])\n",
    "        dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "    names = dtf_features[\"feature\"].unique().tolist()\n",
    "\n",
    "    ## Check the main ingredients\n",
    "    for cat in np.unique(labels):\n",
    "        print(\"# {}:\".format(cat))\n",
    "        print(\"  . selected features:\",len(dtf_features[dtf_features[\"labels\"]==cat]))\n",
    "        print(\"  . top features:\", \",\".join(dtf_features[dtf_features[\"labels\"]==cat][\"feature\"].values[:10]))\n",
    "        print(\" \")\n",
    "    \n",
    "    ## New embeddings\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=names)\n",
    "    vectorizer.fit(corpus)\n",
    "    embedded_ingredients = vectorizer.transform(corpus)\n",
    "    dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83ba94",
   "metadata": {},
   "source": [
    "6. Pre-Process Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd924fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recipes(data): # Recipes dataset\n",
    "    # list of stopwords\n",
    "    stop_word_list = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # Extend list of stop words\n",
    "    stop_word_list.extend(additional_stop_words)\n",
    "\n",
    "    data[\"ingredients_query\"] = data[\"ingredients\"].apply(lambda x: \n",
    "            utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "            lst_stopwords=stop_word_list))\n",
    "    return data\n",
    "\n",
    "def get_tokenize_text(input_text):\n",
    "    # list of stopwords\n",
    "    stop_word_list = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # Extend list of stop words\n",
    "    stop_word_list.extend(additional_stop_words)\n",
    "\n",
    "    return utils_preprocess_text(input_text, flg_stemm=False, flg_lemm=True, lst_stopwords=stop_word_list) # same function to pre-process text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ca156",
   "metadata": {},
   "source": [
    "recipes-telegram-bot/src/recommendation_engine/create_model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3031006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "## model & processing libraries\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "## DB accesses\n",
    "import sqlite3 as sq\n",
    "#from src.recommendation_engine.feature_engineering import process_data, create_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc8e5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/nlp\"\n",
    "MODEL_EMBEDDINGS_PATH = os.path.join(MODEL_PATH, 'similarity_embeddings')\n",
    "\n",
    "CUISINE_CLASSES = ['brazilian','british','cajun_creole','chinese','filipino','french','greek','indian',\n",
    "                   'irish','italian','jamaican','japanese','korean','mexican','moroccan','russian','southern_us',\n",
    "                   'spanish','thai','vietnamese']\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_EMBEDDINGS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a0ac876",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to file in the current working directory\n",
    "def save_pkl(file, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as pkl_file:\n",
    "        pickle.dump(file, pkl_file)\n",
    "\n",
    "def compute_performances(predicted, predicted_prob, y_test):\n",
    "    \n",
    "    classes = np.unique(y_test)\n",
    "    y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "\n",
    "    ## Accuracy, Precision, Recall\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    balance_accuracy = metrics.balanced_accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                                multi_class=\"ovr\")\n",
    "    print(\"Balanced Accuracy:\",  round(balance_accuracy,2))\n",
    "    print(\"Accuracy:\",  round(accuracy,2))\n",
    "    print(\"Auc:\", round(auc,2))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09282cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cuisine_predictions():\n",
    "    ## Process data\n",
    "    dataset = process_data() # cuisine dataset\n",
    "\n",
    "    ## Create embeddings\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer() #create_embeddings(dataset)\n",
    "\n",
    "    ## Model\n",
    "    classifier = LogisticRegressionCV(cv=3,\n",
    "                                      random_state=42,\n",
    "                                      max_iter=300,\n",
    "                                      n_jobs=-1,\n",
    "                                      verbose=1) #naive_bayes.MultinomialNB()\n",
    "\n",
    "    ## pipeline\n",
    "    model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                                (\"classifier\", classifier)])\n",
    "\n",
    "    ## Split the dataset\n",
    "    dataset_train, dataset_test = model_selection.train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "    ## Create embeddings\n",
    "    X_train = dataset_train['ingredients_query']; X_test = dataset_test['ingredients_query'];\n",
    "    y_train = dataset_train['cuisine']; y_test = dataset_test['cuisine']; \n",
    "\n",
    "    ## train classifier\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    ## test\n",
    "    predicted = model.predict(X_test)\n",
    "    predicted_prob = model.predict_proba(X_test)\n",
    "\n",
    "    ## Compute performance of the model\n",
    "    compute_performances(predicted, predicted_prob, y_test)\n",
    "    \n",
    "    ## Save model and vectorizer to disk\n",
    "    save_pkl(model, os.path.join(MODEL_PATH, \"pickle_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "781adf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.68\n",
      "Accuracy: 0.79\n",
      "Auc: 0.98\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.76      0.54      0.63       147\n",
      "     british       0.66      0.48      0.56       242\n",
      "cajun_creole       0.78      0.67      0.72       495\n",
      "     chinese       0.78      0.86      0.81       760\n",
      "    filipino       0.71      0.59      0.64       200\n",
      "      french       0.61      0.64      0.63       820\n",
      "       greek       0.79      0.71      0.75       354\n",
      "      indian       0.88      0.91      0.89       899\n",
      "       irish       0.63      0.48      0.54       207\n",
      "     italian       0.81      0.90      0.85      2351\n",
      "    jamaican       0.89      0.67      0.76       135\n",
      "    japanese       0.83      0.70      0.76       443\n",
      "      korean       0.84      0.75      0.79       249\n",
      "     mexican       0.91      0.92      0.92      1957\n",
      "    moroccan       0.83      0.74      0.78       237\n",
      "     russian       0.55      0.43      0.48       144\n",
      " southern_us       0.70      0.79      0.74      1289\n",
      "     spanish       0.61      0.50      0.55       282\n",
      "        thai       0.80      0.77      0.79       484\n",
      "  vietnamese       0.72      0.56      0.63       238\n",
      "\n",
      "    accuracy                           0.79     11933\n",
      "   macro avg       0.75      0.68      0.71     11933\n",
      "weighted avg       0.79      0.79      0.78     11933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_model_cuisine_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64f09199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_embeddings(data): # Recipe dataset\n",
    "    data = data['ingredients_query'].tolist()\n",
    "    tagged_data = [TaggedDocument(words=row.split(), tags=[str(index)]) for index, row in enumerate(data)]\n",
    "\n",
    "    # hyperparmeters (tune later?)\n",
    "    max_epochs = 20\n",
    "    vec_size = 50\n",
    "    alpha = 0.025\n",
    "\n",
    "    model_embedding = Doc2Vec(vector_size=vec_size,\n",
    "                        alpha=alpha, \n",
    "                        min_alpha=0.00025,\n",
    "                        min_count=1,\n",
    "                        dm =1)\n",
    "  \n",
    "    model_embedding.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model_embedding.train(tagged_data,\n",
    "                    total_examples=model_embedding.corpus_count,\n",
    "                    epochs=model_embedding.epochs)\n",
    "        # decrease the learning rate\n",
    "        model_embedding.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model_embedding.min_alpha = model_embedding.alpha\n",
    "    \n",
    "    return model_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b943f8",
   "metadata": {},
   "source": [
    "recipes-telegram-bot/src/data_base/generate_db.py /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac266f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to move this chunck down \n",
    "# Database Part I\n",
    "\n",
    "import sqlite3 as sq\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#from src.recommendation_engine.data import import_recipes_main\n",
    "#from src.recommendation_engine.feature_engineering import process_recipes\n",
    "#from src.recommendation_engine.inference import load_pkl\n",
    "\n",
    "MODEL_PATH = 'models/nlp'\n",
    "\n",
    "def create_and_populate_db():\n",
    "    data = import_recipes_main_test()\n",
    "    \n",
    "    # Process the data\n",
    "    data = process_recipes(data)\n",
    "    \n",
    "    # Predict cuisine from trained model\n",
    "    model = load_pkl(os.path.join(MODEL_PATH, 'pickle_model.pkl'))\n",
    "    data[\"cuisine\"] = model.predict(data[\"ingredients_query\"].tolist())\n",
    "    \n",
    "    db = sq.connect('recipes.db')\n",
    "    #Verify dtypes\n",
    "    for col in data.columns:\n",
    "        data[col] = data[col].astype('str')\n",
    "\n",
    "    print(' ------------------ Check data before populating the db ------------------')\n",
    "    print(data.columns)\n",
    "    print(data.head())\n",
    "    print(data.shape)\n",
    "    data.to_sql('main_recipes', db, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ade73107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Part II\n",
    "\n",
    "import sqlite3 as sq\n",
    "import pandas as pd\n",
    "\n",
    "def get_df_from_db(cuisine):\n",
    "    db = sq.connect('recipes.db')\n",
    "    sql_query = \"SELECT title, instructions, ingredients, ingredients_query FROM main_recipes WHERE cuisine = ?\"\n",
    "    return pd.read_sql(sql_query, db, params=(cuisine,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31cead27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------------ Check data before populating the db ------------------\n",
      "Index(['title', 'ingredients', 'instructions', 'ingredients_query', 'cuisine'], dtype='object')\n",
      "                               title  \\\n",
      "0  Slow Cooker Chicken and Dumplings   \n",
      "1      Awesome Slow Cooker Pot Roast   \n",
      "2               Brown Sugar Meatloaf   \n",
      "3        Best Chocolate Chip Cookies   \n",
      "4  Homemade Mac and Cheese Casserole   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  ['4 skinless, boneless chicken breast halves A...   \n",
      "1  ['2 (10.75 ounce) cans condensed cream of mush...   \n",
      "2  ['1/2 cup packed brown sugar ADVERTISEMENT', '...   \n",
      "3  ['1 cup butter, softened ADVERTISEMENT', '1 cu...   \n",
      "4  ['8 ounces whole wheat rotini pasta ADVERTISEM...   \n",
      "\n",
      "                                        instructions  \\\n",
      "0  Place the chicken, butter, soup, and onion in ...   \n",
      "1  In a slow cooker, mix cream of mushroom soup, ...   \n",
      "2  Preheat oven to 350 degrees F (175 degrees C)....   \n",
      "3  Preheat oven to 350 degrees F (175 degrees C)....   \n",
      "4  Preheat oven to 350 degrees F. Line a 2-quart ...   \n",
      "\n",
      "                                   ingredients_query      cuisine  \n",
      "0   skinless boneless chicken breast half butter ...  southern_us  \n",
      "1   can condensed cream mushroom soup package dry...      italian  \n",
      "2   packed brown sugar ketchup lean ground beef m...  southern_us  \n",
      "3   butter softened white sugar packed brown suga...  southern_us  \n",
      "4   whole wheat rotini pasta fresh broccoli flore...      italian  \n",
      "(124647, 5)\n"
     ]
    }
   ],
   "source": [
    "create_and_populate_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc817ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_embeddings():\n",
    "    db = sq.connect('recipes.db')\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    for cuisine in CUISINE_CLASSES:\n",
    "        sql_query = \"SELECT title, instructions, ingredients, ingredients_query FROM main_recipes WHERE cuisine = ?\"\n",
    "        data = pd.read_sql(sql_query, db, params=(cuisine,))\n",
    "        \n",
    "        model_embedding = d2v_embeddings(data)\n",
    "        save_pkl(model_embedding, os.path.join(MODEL_EMBEDDINGS_PATH, f'd2v_{cuisine}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d8db847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n"
     ]
    }
   ],
   "source": [
    "train_model_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b95c1e",
   "metadata": {},
   "source": [
    "recipes-telegram-bot/src/recommendation_engine/inference.py /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "337a3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = 'models/nlp'\n",
    "MODEL_EMBEDDINGS_PATH = os.path.join(MODEL_PATH, 'similarity_embeddings')\n",
    "CUISINE_CLASSES = ['greek','southern_us','filipino','indian','jamaican','spanish','italian','mexican','chinese','british','thai','vietnamese','cajun_creole','brazilian','french','japanese','irish','korean','moroccan','russian']\n",
    "\n",
    "#from src.recommendation_engine.feature_engineering import get_tokenize_text\n",
    "#from src.data_base.inference import get_df_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e24750ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from file\n",
    "def load_pkl(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as pkl_file:\n",
    "        return pickle.load(pkl_file)\n",
    "\n",
    "def infer_cuisine_type_on_recipes(data):\n",
    "    model_path = os.path.join(MODEL_PATH, 'pickle_model.pkl')\n",
    "    model = load_pkl(model_path)\n",
    "    data[\"cuisine\"] = model.predict(data[\"ingredients_query\"])\n",
    "    return data\n",
    "    \n",
    "def predict_cuisine(input_text):\n",
    "    top = 5\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokenize_text = get_tokenize_text(input_text)\n",
    "    \n",
    "    # Get model\n",
    "    model_path = os.path.join(MODEL_PATH, 'pickle_model.pkl')\n",
    "    model = load_pkl(model_path)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokenize_text = get_tokenize_text(input_text)\n",
    "\n",
    "    # Get classes ordered by probability\n",
    "    proba = model.predict_proba([tokenize_text])[0]\n",
    "\n",
    "    # Sorted index list \n",
    "    indexes = sorted(range(len(proba)), key=lambda k: proba[k], reverse=True)\n",
    "\n",
    "    # Get cuisine\n",
    "    cuisine_labels = model.classes_.tolist()\n",
    "    cusine_ordered = [cuisine_labels[ind] for ind in indexes]\n",
    "\n",
    "    return cusine_ordered[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "484cc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_recipes(input_text, cuisine, top_k=3):\n",
    "    # Tokenize text\n",
    "    tokenize_text = get_tokenize_text(input_text).split()\n",
    "    \n",
    "    # Load model from the selected cuisine\n",
    "    d2v = load_pkl(os.path.join(MODEL_EMBEDDINGS_PATH, f'd2v_{cuisine}.pkl'))\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = d2v.infer_vector(tokenize_text)\n",
    "    best_recipes = d2v.docvecs.most_similar([embeddings]) #gives you top 10 document tags and their cosine similarity\n",
    "\n",
    "    # Get recipes\n",
    "    best_recipes_index = [int(output[0]) for output in best_recipes]\n",
    "    \n",
    "    # Get dDtaFrame\n",
    "    df = get_df_from_db(cuisine)\n",
    "    \n",
    "    return df[df.index.isin(best_recipes_index)].head(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f7dc4",
   "metadata": {},
   "source": [
    "recipes-telegram-bot/src/data_base/inference.py /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f4446",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e9bd955",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8da3149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting telegram\n",
      "  Downloading telegram-0.0.1.tar.gz (879 bytes)\n",
      "Building wheels for collected packages: telegram\n",
      "  Building wheel for telegram (setup.py): started\n",
      "  Building wheel for telegram (setup.py): finished with status 'done'\n",
      "  Created wheel for telegram: filename=telegram-0.0.1-py3-none-any.whl size=1307 sha256=17db558bbc565f494c67a7d497ae41572be9aaf8540bb214f898f7347b5154cb\n",
      "  Stored in directory: c:\\users\\jsoba\\appdata\\local\\pip\\cache\\wheels\\11\\7a\\5d\\62391dcb6b9a45247192c3a711bf03ed513c14218a8b275a63\n",
      "Successfully built telegram\n",
      "Installing collected packages: telegram\n",
      "Successfully installed telegram-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\jsoba\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49dda545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "215dc7fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ReplyKeyboardMarkup' from 'telegram' (C:\\Users\\jsoba\\anaconda3\\lib\\site-packages\\telegram\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10368/1814295126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtelegram\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReplyKeyboardMarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInlineKeyboardMarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInlineKeyboardButton\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtelegram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUpdater\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCommandHandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMessageHandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConversationHandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallbackContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallbackQueryHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ReplyKeyboardMarkup' from 'telegram' (C:\\Users\\jsoba\\anaconda3\\lib\\site-packages\\telegram\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import argparse\n",
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "from telegram import ReplyKeyboardMarkup, Update, InlineKeyboardMarkup, InlineKeyboardButton\n",
    "from telegram.ext import (Updater, CommandHandler, MessageHandler, Filters, ConversationHandler, CallbackContext, CallbackQueryHandler)\n",
    "\n",
    "#from src.recommendation_engine.inference import predict_cuisine, get_similar_recipes\n",
    "#from src.recognition_engine.inference import classify_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e95bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used: predict_cuisine, get_similar_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parser\n",
    "my_parser = argparse.ArgumentParser(description='Give your personal token')\n",
    "\n",
    "# Add the arguments\n",
    "my_parser.add_argument('token', metavar='token', type=str, help='The token given by Fatherbot')\n",
    "\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    #filename= 'telgramBot.log',\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Main interactions ----\n",
    "CHOOSING, GET_TEXT, GET_IMAGE = range(3)\n",
    "# Callback data\n",
    "CALLBACK1, CALLBACK2 = range(3,5)\n",
    "\n",
    "reply_keyboard = [\n",
    "    ['Show ingredients', 'Get recipes'],\n",
    "    ['Remove item', 'Done'],\n",
    "]\n",
    "markup = ReplyKeyboardMarkup(reply_keyboard, one_time_keyboard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: Start\")\n",
    "\n",
    "    context.user_data['chat_id'] = update.message.chat_id\n",
    "\n",
    "    update.message.reply_text(\n",
    "        \"Hi! I am you recipe bot. What ingredients do you currently have?\"\n",
    "        \"You can send an image or add ingredients by typing it in one or two words\",\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def get_basket_txt(list_ingredients):\n",
    "    txt = 'Here are your current ingredients:\\n'\n",
    "    for ingredient in list_ingredients:\n",
    "        txt += f'   - {ingredient}\\n'\n",
    "    return txt\n",
    "\n",
    "\n",
    "\n",
    "def button1(update: Update, context: CallbackContext) -> int:\n",
    "    logger.info(f\": button1\")\n",
    "\n",
    "    query = update.callback_query\n",
    "    query.answer()\n",
    "\n",
    "    user_data = context.user_data    \n",
    "    if 'ingredients_list' not in user_data:\n",
    "        user_data['ingredients_list'] = [query.data]\n",
    "    else:\n",
    "        user_data['ingredients_list'].append(query.data)\n",
    "\n",
    "    query.edit_message_text(text=f\"Ok you selected: {query.data}\")\n",
    "    \n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    context.bot.send_message(chat_id=context.user_data['chat_id'], text=txt)\n",
    "\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def recipes_query(update: Update, context: CallbackContext) -> int:\n",
    "    \"\"\" Get recipes \"\"\"\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: recipes_query\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "\n",
    "    input_text = ' '.join(user_data['ingredients_list'])\n",
    "\n",
    "    # Predict cuisine\n",
    "    cuisine = predict_cuisine(input_text)\n",
    "\n",
    "    keyboard = [\n",
    "        [\n",
    "            InlineKeyboardButton(cuisine[0], callback_data=cuisine[0]),\n",
    "            InlineKeyboardButton(cuisine[1], callback_data=cuisine[1]),\n",
    "            InlineKeyboardButton(cuisine[2], callback_data=cuisine[2])],\n",
    "        [\n",
    "            InlineKeyboardButton(cuisine[3], callback_data=cuisine[3]),\n",
    "            InlineKeyboardButton(cuisine[4], callback_data=cuisine[4]),\n",
    "        ]\n",
    "    ]\n",
    "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
    "    # Send message with text and appended InlineKeyboard\n",
    "    update.message.reply_text(\"Chose the type of cuisine you want!\", reply_markup=reply_markup)\n",
    "\n",
    "    return CALLBACK2\n",
    "\n",
    "\n",
    "def button2(update: Update, context: CallbackContext) -> int:\n",
    "    #user = update.message.from_user\n",
    "    logger.info(f\"button2\")\n",
    "\n",
    "    query = update.callback_query\n",
    "    query.answer()\n",
    "\n",
    "    # Get recipes\n",
    "    recipes = get_similar_recipes(context.user_data['ingredients_list'], query.data)\n",
    "\n",
    "    sep = '\\n\\n'\n",
    "    for index, row in recipes.iterrows():\n",
    "\n",
    "        title = 'Title: ' + row['title'] \n",
    "        ingredients=''\n",
    "        list_ing = row['ingredients'].replace('ADVERTISEMENT', '').strip('][').split(', ')\n",
    "        for ingredient in list_ing:\n",
    "            ingredients+= ingredient.replace(\"'\", \"\") + '\\n'\n",
    "        ingredients = 'Ingredients: ' + '\\n' + ingredients\n",
    "        instructions = 'Instruction: '+ '\\n' + row['instructions']\n",
    "\n",
    "        txt = title + sep + ingredients + sep + instructions\n",
    "\n",
    "        context.bot.send_message(chat_id=context.user_data['chat_id'], text=txt)\n",
    "\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def show_basket(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: show_basket\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    \n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    \n",
    "    update.message.reply_text(\n",
    "        txt,\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def received_text_information(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: received_text_information\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    text = update.message.text\n",
    "    \n",
    "    if 'ingredients_list' not in user_data:\n",
    "        user_data['ingredients_list'] = [text]\n",
    "    else:\n",
    "        user_data['ingredients_list'].append(text)\n",
    "\n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    update.message.reply_text(\n",
    "        txt,\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def remove_item(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: remove_item\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    if 'ingredients_list' in user_data:\n",
    "        del user_data['ingredients_list'][-1]\n",
    "    \n",
    "    introduction = 'You have deleted the last ingredient. '\n",
    "    txt = get_basket_txt(user_data['ingredients_list'])\n",
    "    update.message.reply_text(\n",
    "        introduction + txt,\n",
    "        reply_markup=markup,\n",
    "    )\n",
    "    return CHOOSING\n",
    "\n",
    "\n",
    "def done(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    logger.info(f\"{user.first_name}: done\")\n",
    "\n",
    "    user_data = context.user_data\n",
    "    if 'ingredients_list' in user_data:\n",
    "        del user_data['ingredients_list']\n",
    "\n",
    "    update.message.reply_text(\n",
    "        f\"Bye bye until next time!\"\n",
    "    )\n",
    "\n",
    "    user_data.clear()\n",
    "    return ConversationHandler.END    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(bot_token) -> None:\n",
    "    # Create the Updater and pass it your bot's token.\n",
    "    # Make sure to set use_context=True to use the new context based callbacks\n",
    "    # Post version 12 this will no longer be necessary\n",
    "    updater = Updater(bot_token, use_context=True)\n",
    "\n",
    "    # Get the dispatcher to register handlers\n",
    "    dispatcher = updater.dispatcher\n",
    "\n",
    "    # Add conversation handler with the states CHOOSING, TYPING_CHOICE and TYPING_REPLY\n",
    "    conv_handler = ConversationHandler(\n",
    "        entry_points=[CommandHandler('start', start)],\n",
    "        states={\n",
    "            CHOOSING: [\n",
    "                MessageHandler(Filters.photo & ~(Filters.command | Filters.regex('^(Done|Get recipes|Show ingredients|Remove item)$')), received_image_information),\n",
    "                MessageHandler(Filters.text & ~(Filters.command | Filters.regex('^(Done|Get recipes|Show ingredients|Remove item)$')), received_text_information),\n",
    "                MessageHandler(Filters.regex('^Get recipes$'), recipes_query),\n",
    "                MessageHandler(Filters.regex('^Show ingredients$'), show_basket),\n",
    "                MessageHandler(Filters.regex('^Remove item$'), remove_item),\n",
    "            ],\n",
    "            CALLBACK1: [\n",
    "                CallbackQueryHandler(button1)],\n",
    "            CALLBACK2: [\n",
    "                CallbackQueryHandler(button2)],\n",
    "        },\n",
    "        fallbacks=[MessageHandler(Filters.regex('^Done$'), done)],\n",
    "        per_message=False,\n",
    "    )\n",
    "\n",
    "    dispatcher.add_handler(conv_handler)\n",
    "\n",
    "    # Start the Bot\n",
    "    updater.start_polling()\n",
    "\n",
    "    # Run the bot until you press Ctrl-C or the process receives SIGINT,\n",
    "    # SIGTERM or SIGABRT. This should be used most of the time, since\n",
    "    # start_polling() is non-blocking and will stop the bot gracefully.\n",
    "    updater.idle()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Execute the parse_args() method\n",
    "    args = my_parser.parse_args()\n",
    "    main(args.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f828bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def received_image_information(update: Update, context: CallbackContext) -> int:\n",
    "    user = update.message.from_user\n",
    "    photo_file = update.message.photo[-1].get_file()\n",
    "    photo_file.download('infer_image.png')\n",
    "    logger.info(\"Photo of %s: %s\", user.first_name, 'infer_image.jpg')\n",
    "    update.message.reply_text(\n",
    "        'Thanks the photo is being processed'\n",
    "    )\n",
    "\n",
    "    user_data = context.user_data\n",
    "    \n",
    "    # Infer image prediction\n",
    "    ingredient = classify_image('infer_image.png')\n",
    "    \n",
    "    keyboard = [\n",
    "        [\n",
    "            InlineKeyboardButton(ingredient[0], callback_data=ingredient[0]),\n",
    "            InlineKeyboardButton(ingredient[1], callback_data=ingredient[1]),\n",
    "            InlineKeyboardButton(ingredient[2], callback_data=ingredient[2])],\n",
    "        [\n",
    "            InlineKeyboardButton(ingredient[3], callback_data=ingredient[3]),\n",
    "            InlineKeyboardButton(ingredient[4], callback_data=ingredient[4]),\n",
    "        ]\n",
    "    ]\n",
    "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
    "    # Send message with text and appended InlineKeyboard\n",
    "    update.message.reply_text(\"Chose the ingredients you have on your image!\", reply_markup=reply_markup)\n",
    "\n",
    "    return CALLBACK1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
