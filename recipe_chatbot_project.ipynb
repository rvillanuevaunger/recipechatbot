{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Recipe Chatbot Project</center></h1>\n",
    "<h2><center>DATA-641</center></h2>\n",
    "<h3><center>Sobanaa Jayakumar & Reina Villanueva-Unger</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "### A. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#feature engg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## model & processing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "## DB accesses\n",
    "import sqlite3 as sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Set Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CUISINE_PATH = \"data/cuisine_data/\"\n",
    "DATA_RECIPES_PATH = \"data/recipes_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/nlp\"\n",
    "MODEL_EMBEDDINGS_PATH = os.path.join(MODEL_PATH, 'similarity_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def import_recipe_data(): # import_recipes_main()\\n    all_recipes = os.path.join(DATA_RECIPES_PATH, \"recipes_raw_nosource_ar.json\", orient=\\'index\\')\\n    epicurious = os.path.join(DATA_RECIPES_PATH, \"recipes_raw_nosource_epi.json\", orient=\\'index\\')\\n    food_network = os.path.join(DATA_RECIPES_PATH, \"recipes_raw_nosource_fn.json\", orient=\\'index\\')\\n    \\n    data =  pd.concat([all_recipes, epicurious, food_network], axis=0)\\n    data = data.reset_index()\\n    data = data.drop(columns=[\\'picture_link\\', \\'index\\'])\\n    return data'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuisine Data\n",
    "def import_cuisine_data(): # import_data()\n",
    "    train = pd.read_json(os.path.join(DATA_CUISINE_PATH, 'train.json'))\n",
    "    return train\n",
    "    #test = pd.read_json(os.path.join(DATA_CUISINE_PATH, 'test.json'))\n",
    "    #return pd.concat([train,test],axis=0)\n",
    "\n",
    "# Recipe Data \n",
    "\n",
    "def import_recipe_data():\n",
    "    all_recipes = pd.read_json('./data/recipes_data/recipes_raw_nosource_allrecipes.json', orient='index')\n",
    "    epicurious = pd.read_json('./data/recipes_data/recipes_raw_nosource_epicurious.json', orient='index')\n",
    "    food_network = pd.read_json('./data/recipes_data/recipes_raw_nosource_foodnetwork.json', orient='index')\n",
    "    data = pd.concat([all_recipes, epicurious, food_network], axis=0)\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    data = data.drop(columns=['index', 'picture_link'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Set Cuisine Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUISINE_CLASSES = ['greek','southern_us','filipino','indian','jamaican','spanish','italian','mexican','chinese','british','thai','vietnamese','cajun_creole','brazilian','french','japanese','irish','korean','moroccan','russian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stop_words = [\"advertisement\", \"advertisements\",\n",
    "                         \"cup\", \"cups\",\n",
    "                         \"tablespoon\", \"tablespoons\", \n",
    "                         \"teaspoon\", \"teaspoons\", \n",
    "                         \"ounce\", \"ounces\",\n",
    "                         \"salt\", \n",
    "                         \"pepper\", \n",
    "                         \"pound\", \"pounds\",\n",
    "                         ]\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.extend(additional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. String Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. String Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def clean_string(list, lemmatize = True, stemming = False, lst_stopwords=None):\n",
    "    str = ' '.join(list) #converting the list to string\n",
    "    clean_text = ''\n",
    "    \n",
    "    lower = str.lower().split() #lowercase and tokenize\n",
    "    \n",
    "    lst_text=[]\n",
    "    clean_words = []\n",
    "    for word in lower:\n",
    "        if len(word) > 2:\n",
    "            digit = re.sub(r'\\d+','', word) #removing digits\n",
    "            text = re.sub(r'[^\\w\\s]', '', digit) #removing punc and characters            \n",
    "            \n",
    "            if lemmatize:\n",
    "                lm = nltk.stem.wordnet.WordNetLemmatizer()  #lemmatize\n",
    "                lemm = lm.lemmatize(text)\n",
    "                clean_words.append(lemm)\n",
    "                \n",
    "                if stemming:\n",
    "                    stemmer = nltk.stem.porter.PorterStemmer #stemming\n",
    "                    stemm = stemmer.stem(text)\n",
    "                    clean_words.append(stemm)\n",
    "         \n",
    "    #rem_stop = [i for i in clean_words if i not in stopword_list]  #remove stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "    \n",
    "    clean_text = ' '.join(lst_text) #join as a string\n",
    "    text = re.sub(' +', ' ', clean_text) #remove multi-spaces\n",
    "    \n",
    "    return text  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this function!!!\n",
    "def clean_string(text, stemming=False, lemmatize=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    if len(text) > 2:\n",
    "        lst_text = text.split()\n",
    "\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if stemming == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if lemmatize == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "\n",
    "    ## Remove digits\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    ## remove mutliple space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' romaine lettuce black olive grape tomato plain flour ground tomato g egg mayonaise cooking oil g water vegetable oil wheat black shallot cornflour cayenne pe light brown sugar granulated sugar butter kraft zesty italian dressing purple onion b egg citrus fruit raisin sourdough starte boneless chicken skinless thigh minced garli green chile jalapeno chilies onion ground name ingredient length dtype object'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisine_df = import_cuisine_data()\n",
    "clean_string(cuisine_df['ingredients'], lst_stopwords=stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Preprocess Cuisine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cuisine(): # !!!--- NEED TO UPDATE LATER IN CODE ---!!! originally: process_data()\n",
    "    dataset = import_cuisine_data() # cuisine\n",
    "\n",
    "    def processing(row):\n",
    "        ls = row['ingredients']\n",
    "        return ' '.join(ls)\n",
    "\n",
    "    dataset['ingredients'] = dataset.apply(lambda x: processing(x), axis=1)\n",
    "    dataset.dropna(inplace=True)\n",
    "    dataset = dataset.drop(columns=['id']).reset_index(drop=True)\n",
    "\n",
    "    dataset[\"ingredients_query\"] = dataset[\"ingredients\"].apply(lambda x: \n",
    "          clean_string(x, stemming=False, lemmatize=True, lst_stopwords=stopword_list)\n",
    "          )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Preprocess Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recipes(data): # Recipes dataset\n",
    "    data[\"ingredients_query\"] = data[\"ingredients\"].apply(lambda x: \n",
    "            clean_string(x, stemming=False, lemmatize=True, lst_stopwords=stopword_list))\n",
    "    return data\n",
    "\n",
    "def get_tokenize_text(input_text):\n",
    "    return clean_string(input_text, stemming=False, lemmatize=True, lst_stopwords=stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Word Embeddings from Cuisine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def create_embeddings(dataset):\n",
    "    ## Tf-Idf (advanced variant of BoW)\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "    corpus = dataset[\"ingredients_query\"]\n",
    "    vectorizer.fit(corpus)\n",
    "    embedded_ingredients = vectorizer.transform(corpus)\n",
    "    dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "    ## Chi squarred correlation embeddings reduction\n",
    "    labels = dataset[\"cuisine\"]\n",
    "    names = vectorizer.get_feature_names()\n",
    "    p_value_limit = 0.95\n",
    "    dtf_features = pd.DataFrame()\n",
    "\n",
    "    for cat in np.unique(labels):\n",
    "        chi2, p = feature_selection.chi2(embedded_ingredients, labels==cat)\n",
    "        dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                       {\"feature\":names, \"score\":1-p, \"labels\":cat}))\n",
    "        dtf_features = dtf_features.sort_values([\"labels\",\"score\"], \n",
    "                        ascending=[True,False])\n",
    "        dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "    names = dtf_features[\"feature\"].unique().tolist()\n",
    "\n",
    "    ## Check the main ingredients\n",
    "    for cat in np.unique(labels):\n",
    "        print(\"# {}:\".format(cat))\n",
    "        print(\"  . selected features:\",len(dtf_features[dtf_features[\"labels\"]==cat]))\n",
    "        print(\"  . top features:\", \",\".join(dtf_features[dtf_features[\"labels\"]==cat][\"feature\"].values[:10]))\n",
    "        print(\" \")\n",
    "    \n",
    "    ## New embeddings\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=names)\n",
    "    vectorizer.fit(corpus)\n",
    "    embedded_ingredients = vectorizer.transform(corpus)\n",
    "    dic_vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "    return vectorizer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cuisine Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Tested Multiple Categorization Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobanaa add here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Using the best Model & Evaluating Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Saving Pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(file, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as pkl_file:\n",
    "        pickle.dump(file, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Compute Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performances(predicted, predicted_prob, y_test):\n",
    "    \n",
    "    classes = np.unique(y_test)\n",
    "    y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "\n",
    "    ## Accuracy, Precision, Recall\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    balance_accuracy = metrics.balanced_accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                                multi_class=\"ovr\")\n",
    "    print(\"Balanced Accuracy:\",  round(balance_accuracy,2))\n",
    "    print(\"Accuracy:\",  round(accuracy,2))\n",
    "    print(\"Auc:\", round(auc,2))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Cuisine Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_cuisine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['ingredients_query']\n",
    "y = dataset['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "#Using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Fitting vectorizer\n",
    "matrix_train_lr = vectorizer.fit_transform(X_train)\n",
    "matrix_test_lr = vectorizer.transform(X_test)\n",
    "\n",
    "#Fitting final model with Best hyper params\n",
    "lr_final = LogisticRegression(max_iter = 300, random_state = 123, multi_class ='multinomial', solver = 'newton-cg', C = 10, penalty = 'l2')\n",
    "lr_final.fit(matrix_train_lr, y_train)\n",
    "y_pred = lr_final.predict(matrix_test_lr)\n",
    "pred_prob = lr_final.predict_proba(matrix_test_lr)\n",
    "\n",
    "#Classification metrics\n",
    "print('f1 score weighted %s' % f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print('Accuracy score %s' % accuracy_score(y_test, y_pred))\n",
    "#print('AUC score %s' % roc_auc_score(y_test, pred_prob))\n",
    "#print('ROC score %s' % roc_curve(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#cm_lr_test=confusion_matrix(y_test, y_pred)\n",
    "#print(cm_lr_test)\n",
    "\n",
    "acc_lr = accuracy_score(y_test, y_pred)\n",
    "f1_lr = f1_score(y_test, y_pred, average = 'weighted')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling Above:\n",
    "- Vectorizer\n",
    "- Split Dataset\n",
    "\n",
    "Within Function:\n",
    "- Classifier\n",
    "- Fit vectorizer\n",
    "- Fitting the model\n",
    "- Saving as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cuisine_predictions():\n",
    "    # Classifer\n",
    "    classifier = LogisticRegression(random_state=123,\n",
    "                                      max_iter=300,\n",
    "                                      n_jobs=-1,\n",
    "                                      multi_class = 'multinomial', \n",
    "                                      solver = 'newton-cg', \n",
    "                                      C = 10, \n",
    "                                      penalty = 'l2',\n",
    "                                      verbose=1\n",
    "                                      ) \n",
    "    ## pipeline\n",
    "    model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                                (\"classifier\", classifier)])\n",
    "\n",
    "    #Fitting Vectorizer\n",
    "    #matrix_train_lr = vectorizer.fit_transform(X_train)\n",
    "    #matrix_test_lr = vectorizer.transform(X_test)\n",
    "\n",
    "    # Fit Model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_prob = model.predict_proba(X_train)\n",
    "\n",
    "    # Save as Pkl\n",
    "    save_pkl(model, os.path.join(MODEL_PATH, \"pickle_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.0s finished\n"
     ]
    }
   ],
   "source": [
    "create_model_cuisine_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define load pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from file\n",
    "def load_pkl(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as pkl_file:\n",
    "        return pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/nlp'\n",
    "\n",
    "def create_and_populate_db():\n",
    "    data = import_recipe_data()\n",
    "    \n",
    "    # Process the data\n",
    "    data = process_recipes(data)\n",
    "    \n",
    "    # Predict cuisine from trained model\n",
    "    model = load_pkl(os.path.join(MODEL_PATH, 'pickle_model.pkl'))\n",
    "    #data[\"cuisine\"] = model.predict(data[\"ingredients_query\"].tolist())\n",
    "    \n",
    "    db = sq.connect('recipes.db')\n",
    "    #Verify dtypes\n",
    "    for col in data.columns:\n",
    "        data[col] = data[col].astype('str')\n",
    "\n",
    "    print(' ------------------ Check data before populating the db ------------------')\n",
    "    print(data.columns)\n",
    "    print(data.head())\n",
    "    print(data.shape)\n",
    "    data.to_sql('main_recipes', db, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------------ Check data before populating the db ------------------\n",
      "Index(['title', 'ingredients', 'instructions', 'ingredients_query'], dtype='object')\n",
      "                               title  \\\n",
      "0  Slow Cooker Chicken and Dumplings   \n",
      "1      Awesome Slow Cooker Pot Roast   \n",
      "2               Brown Sugar Meatloaf   \n",
      "3        Best Chocolate Chip Cookies   \n",
      "4  Homemade Mac and Cheese Casserole   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  ['4 skinless, boneless chicken breast halves A...   \n",
      "1  ['2 (10.75 ounce) cans condensed cream of mush...   \n",
      "2  ['1/2 cup packed brown sugar ADVERTISEMENT', '...   \n",
      "3  ['1 cup butter, softened ADVERTISEMENT', '1 cu...   \n",
      "4  ['8 ounces whole wheat rotini pasta ADVERTISEM...   \n",
      "\n",
      "                                        instructions  \\\n",
      "0  Place the chicken, butter, soup, and onion in ...   \n",
      "1  In a slow cooker, mix cream of mushroom soup, ...   \n",
      "2  Preheat oven to 350 degrees F (175 degrees C)....   \n",
      "3  Preheat oven to 350 degrees F (175 degrees C)....   \n",
      "4  Preheat oven to 350 degrees F. Line a 2-quart ...   \n",
      "\n",
      "                                   ingredients_query  \n",
      "0   skinless boneless chicken breast half butter ...  \n",
      "1   can condensed cream mushroom soup package dry...  \n",
      "2   packed brown sugar ketchup lean ground beef m...  \n",
      "3   butter softened white sugar packed brown suga...  \n",
      "4   whole wheat rotini pasta fresh broccoli flore...  \n",
      "(124647, 4)\n"
     ]
    }
   ],
   "source": [
    "create_and_populate_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_db(cuisine):\n",
    "    db = sq.connect('recipes.db')\n",
    "    sql_query = \"SELECT title, instructions, ingredients, ingredients_query FROM main_recipes WHERE cuisine = ?\"\n",
    "    return pd.read_sql(sql_query, db, params=(cuisine,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_embeddings(data): # Recipe dataset\n",
    "    data = data['ingredients_query'].tolist()\n",
    "    tagged_data = [TaggedDocument(words=row.split(), tags=[str(index)]) for index, row in enumerate(data)]\n",
    "\n",
    "    max_epochs = 20\n",
    "    vec_size = 50\n",
    "    alpha = 0.025\n",
    "\n",
    "    model_embedding = Doc2Vec(vector_size=vec_size,\n",
    "                        alpha=alpha, \n",
    "                        min_alpha=0.00025,\n",
    "                        min_count=1,\n",
    "                        dm =1)\n",
    "  \n",
    "    model_embedding.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model_embedding.train(tagged_data,\n",
    "                    total_examples=model_embedding.corpus_count,\n",
    "                    epochs=model_embedding.epochs)\n",
    "        # decrease the learning rate\n",
    "        model_embedding.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model_embedding.min_alpha = model_embedding.alpha\n",
    "    \n",
    "    return model_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_embeddings():\n",
    "    db = sq.connect('recipes.db')\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    for cuisine in CUISINE_CLASSES:\n",
    "        sql_query = \"SELECT title, instructions, ingredients, ingredients_query FROM main_recipes WHERE cuisine = ?\"\n",
    "        data = pd.read_sql(sql_query, db, params=(cuisine,))\n",
    "        \n",
    "        model_embedding = d2v_embeddings(data)\n",
    "        save_pkl(model_embedding, os.path.join(MODEL_EMBEDDINGS_PATH, f'd2v_{cuisine}.pkl'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db968608ecdb27000edf98c5f63e9ccc25d479a7a16895421f990979c50088b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('data641')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
